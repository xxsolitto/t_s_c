{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425ceb85-d9ff-409a-880f-1d0f94c562c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from torchtext import data\n",
    "from torchtext.legacy import data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed922d-7105-492e-861f-e3d842113ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの前処理（Word Embedding）\n",
    "\n",
    "# テキストを単語で分割\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# data field定義\n",
    "TEXT_N  = data.Field(sequential=True,\n",
    "                     lower=True,\n",
    "                     batch_first=True, \n",
    "                     tokenize=tokenizer,\n",
    "                     init_token='<cls>')\n",
    "#SECTION = data.Field(sequential=False, use_vocab=False)\n",
    "#TREND_N = data.Field(sequential=False, use_vocab=False)\n",
    "#PRICE_N = data.Field(sequential=False, use_vocab=False)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "\n",
    "# CSVファイルを読み込み、TabularDatasetオブジェクトの作成\n",
    "train_data, test_data = data.TabularDataset.splits(path ='tweet-transformer/1h',\n",
    "                                                   train='test7_v4.csv',\n",
    "                                                   test ='test7_v4.csv',\n",
    "                                                   format='csv',\n",
    "                                                   skip_header = True,\n",
    "                                                   fields=[('tweet_n', TEXT_N),\n",
    "                                                           #('section', SECTION),\n",
    "                                                           #('trend_n', TREND_N),\n",
    "                                                           #('price_n', PRICE_N),\n",
    "                                                           ('label', LABEL)])\n",
    "print(\"データ読み込み完了\")\n",
    "\n",
    "# 単語辞書の作成\n",
    "TEXT_N.build_vocab(train_data, min_freq=2)\n",
    "vocab = TEXT_N.vocab\n",
    "print('辞書作成完了')\n",
    "\n",
    "# テキストを数値ベクトル化、バッチに分割\n",
    "batch_size = 128\n",
    "train_iter, test_iter = data.BucketIterator.splits((train_data, test_data),\n",
    "                                                   batch_sizes=(batch_size, batch_size),\n",
    "                                                   shuffle=True)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(train_iter))\n",
    "\n",
    "# メモリ解放\n",
    "del train_data,test_data\n",
    "gc.collect()\n",
    "print('メモリ解放')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f7afac-54ab-4475-b276-d3b7cd2fa0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametator for Net\n",
    "#ntokens = len(vocab)  # size of vocabulary\n",
    "ntokens = 3000  # size of vocabulary\n",
    "d_model = 512  # embedding dimension\n",
    "nhead   = 8    # number of heads in nn.MultiheadAttention\n",
    "d_hid   = 2048  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 6    # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "dropout = 0.2  # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4024d1bf-ade2-4b1a-987c-e759fb32d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerモデルの概要\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 ntoken: int,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 d_hid: int,\n",
    "                 nlayers: int,\n",
    "                 dropout: float = 0.5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.emb = nn.Embedding(ntoken, d_model, padding_idx=0)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.classifer = nn.Linear(d_model, 3)\n",
    "        #self.softmax = nn.Softmax()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.classifer.bias.data.zero_()\n",
    "        self.classifer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    #データの流れ\n",
    "    #def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            src: Tensor, shape [batch_size, seq_len]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [batch_size, nclass:3]\n",
    "        '''\n",
    "\n",
    "        embedded = self.emb(src) * math.sqrt(self.d_model)\n",
    "        pos = self.pos_encoder(embedded)\n",
    "        encoder_out = self.transformer_encoder(pos)\n",
    "        x = encoder_out.mean(dim=1)\n",
    "        output = self.classifer(x)\n",
    "        #output = self.softmax(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0419fd72-cc71-4ea6-9c03-14f8bc8118a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PositionalEncodingの概要\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        '''\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9367e57a-678c-4d2e-bc15-5fdfd4df01fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1accd2ff710>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paramator for training & evaluation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(ntokens, d_model, nhead, d_hid, nlayers, dropout).to(device)\n",
    "lr = 1e-3\n",
    "softmax = nn.Softmax(dim=1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a77848dc-a80f-4233-a46d-82d327d1f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(model: nn.Module, train_iter: Tensor):\n",
    "    train_start_time = time.time()\n",
    "    model.train()\n",
    "    num_batches = len(train_iter)\n",
    "    log_interval = math.ceil(num_batches/100)*10\n",
    "    batch_counter = 0\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_count = 0\n",
    "    \n",
    "    for idx, batch in enumerate(iter(train_iter)):\n",
    "        predictions = model(batch.tweet_n.to(device))\n",
    "        prob = softmax(predictions)\n",
    "        labels = batch.label.to(device)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        '''\n",
    "        print('softmax')\n",
    "        print(prob)\n",
    "        print('予測結果')\n",
    "        print(prob.argmax(axis=1))\n",
    "        print('答え')\n",
    "        print(labels)\n",
    "        '''\n",
    "        \n",
    "        correct = prob.argmax(axis=1) == labels\n",
    "        acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "        train_correct += correct.sum().item()\n",
    "        train_count += correct.size(0)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_counter += 1\n",
    "        \n",
    "        if batch_counter % log_interval == 0 or batch_counter == num_batches:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            s_per_batch = (time.time() - train_start_time) / log_interval\n",
    "            cur_loss = train_loss / log_interval\n",
    "            cur_acc = train_correct / train_count\n",
    "            print(f'| epoch {epoch:3d} | {batch_counter:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:1.5f} | s/batch {s_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | accuracy {cur_acc:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93f793e1-ba75-49cf-8fe6-3a497f39d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation (val, test)\n",
    "def evaluate(model: nn.Module, eval_iter: Tensor):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_correct = 0\n",
    "    eval_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(iter(eval_iter)):\n",
    "            predictions = model(batch.tweet_n.to(device))\n",
    "            prob = softmax(predictions)\n",
    "            labels = batch.label.to(device)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            correct = prob.argmax(axis=1) == labels\n",
    "            acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "            eval_correct += correct.sum().item()\n",
    "            eval_count += correct.size(0)\n",
    "            eval_loss += loss.item()\n",
    "        \n",
    "    print(f'| loss {eval_loss}| accuracy {eval_correct / ecal_count} ')\n",
    "        \n",
    "    return eval_loss, eval_correct / eval_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "499797d0-d06b-4c55-8768-3def2b2ee0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-25 13:57:51.487780\n",
      "学習開始\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch   1 |     8/    8 batches | lr 0.00100 | s/batch  0.26 | loss  3.15 | accuracy     0.38\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  2.57s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch   2 |     8/    8 batches | lr 0.00095 | s/batch  0.24 | loss  1.19 | accuracy     0.35\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  2.45s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch   3 |     8/    8 batches | lr 0.00090 | s/batch  0.23 | loss  0.94 | accuracy     0.41\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  2.33s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch   4 |     8/    8 batches | lr 0.00086 | s/batch  0.23 | loss  0.85 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  2.32s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch   5 |     8/    8 batches | lr 0.00081 | s/batch  0.24 | loss  0.86 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  2.41s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch   6 |     8/    8 batches | lr 0.00077 | s/batch  0.25 | loss  0.87 | accuracy     0.41\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  2.52s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch   7 |     8/    8 batches | lr 0.00074 | s/batch  0.24 | loss  0.89 | accuracy     0.45\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  2.43s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch   8 |     8/    8 batches | lr 0.00070 | s/batch  0.24 | loss  0.85 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  2.40s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch   9 |     8/    8 batches | lr 0.00066 | s/batch  0.24 | loss  0.89 | accuracy     0.37\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  2.43s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  10 |     8/    8 batches | lr 0.00063 | s/batch  0.24 | loss  0.90 | accuracy     0.39\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  2.36s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  11 |     8/    8 batches | lr 0.00060 | s/batch  0.24 | loss  0.86 | accuracy     0.45\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  2.39s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  12 |     8/    8 batches | lr 0.00057 | s/batch  0.24 | loss  0.86 | accuracy     0.43\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  2.42s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  13 |     8/    8 batches | lr 0.00054 | s/batch  0.23 | loss  0.85 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  2.28s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  14 |     8/    8 batches | lr 0.00051 | s/batch  0.23 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  2.28s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  15 |     8/    8 batches | lr 0.00049 | s/batch  0.23 | loss  0.85 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  2.32s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  16 |     8/    8 batches | lr 0.00046 | s/batch  0.25 | loss  0.86 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  2.51s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  17 |     8/    8 batches | lr 0.00044 | s/batch  0.24 | loss  0.85 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  2.38s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  18 |     8/    8 batches | lr 0.00042 | s/batch  0.25 | loss  0.85 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  2.48s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  19 |     8/    8 batches | lr 0.00040 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  2.46s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  20 |     8/    8 batches | lr 0.00038 | s/batch  0.23 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  2.31s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  21 |     8/    8 batches | lr 0.00036 | s/batch  0.24 | loss  0.85 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  2.46s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  22 |     8/    8 batches | lr 0.00034 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  2.45s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  23 |     8/    8 batches | lr 0.00032 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  2.54s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  24 |     8/    8 batches | lr 0.00031 | s/batch  0.24 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  2.45s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  25 |     8/    8 batches | lr 0.00029 | s/batch  0.24 | loss  0.85 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  2.39s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  26 |     8/    8 batches | lr 0.00028 | s/batch  0.24 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  2.36s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  27 |     8/    8 batches | lr 0.00026 | s/batch  0.25 | loss  0.85 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  2.49s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  28 |     8/    8 batches | lr 0.00025 | s/batch  0.24 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  2.44s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  29 |     8/    8 batches | lr 0.00024 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  2.49s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  30 |     8/    8 batches | lr 0.00023 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  2.52s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  31 |     8/    8 batches | lr 0.00021 | s/batch  0.25 | loss  0.85 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  2.45s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  32 |     8/    8 batches | lr 0.00020 | s/batch  0.24 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  2.37s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  33 |     8/    8 batches | lr 0.00019 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  2.50s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  34 |     8/    8 batches | lr 0.00018 | s/batch  0.24 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  2.45s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  35 |     8/    8 batches | lr 0.00017 | s/batch  0.24 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  2.36s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  36 |     8/    8 batches | lr 0.00017 | s/batch  0.24 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  2.43s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  37 |     8/    8 batches | lr 0.00016 | s/batch  0.24 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  2.44s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  38 |     8/    8 batches | lr 0.00015 | s/batch  0.25 | loss  0.85 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  2.50s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  39 |     8/    8 batches | lr 0.00014 | s/batch  0.26 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  2.57s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  40 |     8/    8 batches | lr 0.00014 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time:  2.45s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  41 |     8/    8 batches | lr 0.00013 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time:  2.54s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  42 |     8/    8 batches | lr 0.00012 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time:  2.47s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  43 |     8/    8 batches | lr 0.00012 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time:  2.52s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  44 |     8/    8 batches | lr 0.00011 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time:  2.52s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  45 |     8/    8 batches | lr 0.00010 | s/batch  0.24 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time:  2.43s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  46 |     8/    8 batches | lr 0.00010 | s/batch  0.26 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time:  2.56s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  47 |     8/    8 batches | lr 0.00009 | s/batch  0.24 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time:  2.42s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  48 |     8/    8 batches | lr 0.00009 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time:  2.55s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  49 |     8/    8 batches | lr 0.00009 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time:  2.52s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "| epoch  50 |     8/    8 batches | lr 0.00008 | s/batch  0.25 | loss  0.84 | accuracy     0.49\n",
      "-----------------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time:  2.55s | \n",
      "-----------------------------------------------------------------------------------------------\n",
      "2021-10-25 13:59:53.755947\n",
      "経過時間：0:02:02.268167\n",
      "学習終了\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "# training roop\n",
    "best_val_loss = float('inf')\n",
    "epochs = 50\n",
    "best_model = None\n",
    "\n",
    "dt_start = datetime.datetime.now()\n",
    "print(datetime.datetime.now())\n",
    "print('学習開始')\n",
    "print('-' * 95)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_iter)\n",
    "#    val_loss, val_acc = evaluate(model, val_iter)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 95)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | ')\n",
    "#          f'valid loss {val_loss:5.2f} | valid accuracy {val_acc:8.2f}')\n",
    "    print('-' * 95)\n",
    "\n",
    "#    if val_loss < best_val_loss:\n",
    "#        best_val_loss = val_loss\n",
    "#        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()\n",
    " \n",
    "dt_end = datetime.datetime.now()\n",
    "print(datetime.datetime.now())    \n",
    "print(f'経過時間：{dt_end - dt_start}')\n",
    "print('学習終了')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92f130-e955-4ceb-a92b-6db9199de6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "# test\n",
    "test_loss, test_acc = evaluate(best_model, test_iter)\n",
    "\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test accuracy {test_acc:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9b279-ca38-4d79-a472-ca2529ca9bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23e7f3cc-1fc5-4571-838c-1cf331754174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerモデルの概要\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    #データの流れ\n",
    "    def forward(self, src) -> Tensor:\n",
    "        x=src[0]\n",
    "        output = x.mean(dim=1)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3798a5ee-52b7-43bd-8382-04a3e433ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3091ecfa-7b42-4b5d-81a3-60b1b1a2dade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([1.2500, 1.5000, 1.7500, 2.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,1,1,2],[1,1,1,3],[1,1,1,4],[1,1,1,5],[1,1,1,1]],dtype=torch.float)\n",
    "l = [x,x,x]\n",
    "print(x.ndim)\n",
    "out = model(l)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632eb500-a9e2-4d7f-8ea0-9e1a2a2b713f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee7437-06db-4471-a536-867d4a0bf208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
