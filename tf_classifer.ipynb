{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2a7b47-87ee-41be-9255-4b83434d3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchtext\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "from torchtext.legacy import data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a32fac06-8965-4483-837e-52ac1172eaeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xx/b9j_s_jx73s901xznm0f4wy80000gn/T/ipykernel_950/164385893.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mLABEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m train_iter, test_iter = data.BucketIterator.splits((train_txt, test_txt),\n\u001b[0m\u001b[1;32m     20\u001b[0m                                                    batch_size=batch_size)\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, datasets, batch_sizes, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             ret.append(cls(\n\u001b[0m\u001b[1;32m    100\u001b[0m                 datasets[i], batch_size=batch_sizes[i], train=train, **kwargs))\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, sort_key, device, batch_size_fn, train, repeat, shuffle, sort, sort_within_batch)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_within_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_within_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msort_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "max_length = 256\n",
    "\n",
    "TEXT = data.Field(\n",
    "    lower=True, include_lengths=False, batch_first=True\n",
    ")\n",
    "LABEL = data.Field(sequential=False)\n",
    "train_txt = datasets.IMDB(split='train')\n",
    "test_txt = datasets.IMDB(split='test')\n",
    "\n",
    "TEXT.build_vocab(\n",
    "    train_txt,\n",
    "    vectors=torchtext.vocab.GloVe(name=\"6B\", dim=50, max_vectors=50_000),\n",
    "    max_size=50_000,\n",
    ")\n",
    "\n",
    "LABEL.build_vocab(train_txt)\n",
    "\n",
    "train_iter, test_iter = data.BucketIterator.splits((train_txt, test_txt),\n",
    "                                                   batch_size=batch_size)\n",
    "\n",
    "# メモリ解放\n",
    "gc.collect()\n",
    "print('メモリ解放')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ada369-1ef1-4fbd-b719-0991204fe292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(vocab_size, d_model)\n",
    "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float()\n",
    "            * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29ffcd-f024-42a1-9202-cc62d67072a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Text classifier based on a pytorch TransformerEncoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings,\n",
    "        nhead=8,\n",
    "        dim_feedforward=2048,\n",
    "        num_layers=6,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        classifier_dropout=0.1,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        vocab_size, d_model = embeddings.size()\n",
    "        assert d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n",
    "\n",
    "        self.emb = nn.Embedding.from_pretrained(embeddings, freeze=False)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(\n",
    "            d_model=d_model,\n",
    "            dropout=dropout,\n",
    "            vocab_size=vocab_size,\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.emb(x) * math.sqrt(self.d_model)\n",
    "        print(x.shape)\n",
    "        x = self.pos_encoder(x)\n",
    "        print(x.shape)\n",
    "        x = self.transformer_encoder(x)\n",
    "        print(x.shape)\n",
    "        x = x.mean(dim=1)\n",
    "        print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a5a88-5302-4f16-872c-ab7576326ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "model = Net(\n",
    "    TEXT.vocab.vectors,\n",
    "    nhead=5,  # the number of heads in the multiheadattention models\n",
    "    dim_feedforward=50,  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    num_layers=6,\n",
    "    dropout=0.0,\n",
    "    classifier_dropout=0.0,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), lr=lr\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(\"starting\")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"{epoch=}\")\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_count = 0\n",
    "    for idx, batch in enumerate(iter(train_iter)):\n",
    "        predictions = model(batch.text.to(device))\n",
    "        labels = batch.label.to(device) - 1\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        correct = predictions.argmax(axis=1) == labels\n",
    "        acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "        epoch_correct += correct.sum().item()\n",
    "        epoch_count += correct.size(0)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_epoch_loss = 0\n",
    "        test_epoch_correct = 0\n",
    "        test_epoch_count = 0\n",
    "\n",
    "        for idx, batch in enumerate(iter(test_iter)):\n",
    "            predictions = model(batch.text.to(device))\n",
    "            labels = batch.label.to(device) - 1\n",
    "            test_loss = criterion(predictions, labels)\n",
    "\n",
    "            correct = predictions.argmax(axis=1) == labels\n",
    "            acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "            test_epoch_correct += correct.sum().item()\n",
    "            test_epoch_count += correct.size(0)\n",
    "            test_epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"{epoch_loss=}\")\n",
    "    print(f\"epoch accuracy: {epoch_correct / epoch_count}\")\n",
    "    print(f\"{test_epoch_loss=}\")\n",
    "    print(f\"test epoch accuracy: {test_epoch_correct / test_epoch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdfb92e-bef7-488f-b95a-3e19acad3709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab31e2f8-adea-4b10-be53-ddbad426d877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.datasets_utils._RawTextIterableDataset at 0x10c94ca60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c1b154-9e34-4991-8eab-54b494efdd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2058, -0.0742,  1.3366,  1.3647],\n",
      "        [-1.2786, -0.7082, -2.2737,  0.5185],\n",
      "        [ 0.1826,  0.4152,  1.0471, -1.6233],\n",
      "        [ 0.4471, -1.0526,  0.0310,  0.0494],\n",
      "        [-1.5965,  0.2187,  0.9989,  0.3079]])\n",
      "tensor([[[-1.5523, -1.2434, -0.9419],\n",
      "         [-0.2383,  1.5912, -1.6502],\n",
      "         [-0.2902, -0.4473,  0.2467],\n",
      "         [-2.4460,  0.0074, -1.9800]],\n",
      "\n",
      "        [[-1.2749, -0.5333, -0.5976],\n",
      "         [ 0.8122, -0.1148, -0.5398],\n",
      "         [ 1.6435,  0.2978, -1.7259],\n",
      "         [ 0.5027,  2.0871,  0.2007]],\n",
      "\n",
      "        [[-0.3738,  0.3048,  0.3817],\n",
      "         [ 0.6630,  0.1353, -0.2309],\n",
      "         [-0.7171, -0.7788, -0.5427],\n",
      "         [-0.6018, -0.9397,  1.2979]],\n",
      "\n",
      "        [[-1.5348,  2.4105, -0.9706],\n",
      "         [-0.7284, -1.3929, -0.1321],\n",
      "         [-0.8072, -1.1986, -1.5206],\n",
      "         [-0.5665,  0.4873, -1.8529]],\n",
      "\n",
      "        [[-0.9457,  0.9937, -1.2628],\n",
      "         [ 2.2014,  0.3812, -0.0614],\n",
      "         [ 0.1611,  1.6365,  0.0673],\n",
      "         [ 0.8607, -0.9564,  0.1719]]])\n"
     ]
    }
   ],
   "source": [
    "t=torch.randn(5,4)\n",
    "g=torch.randn(5,4,3)\n",
    "print(t)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e28b0049-f89d-43c2-b73d-95bb145660f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0642)\n",
      "tensor([-0.0958, -0.9968, -0.1704,  1.0062])\n",
      "tensor([-0.0338,  0.4460,  0.5026, -0.1416, -1.0940])\n"
     ]
    }
   ],
   "source": [
    "print(t.mean())\n",
    "print(t.mean(dim=0))\n",
    "print(t.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db951010-703d-4bec-b043-d51a26efeb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
