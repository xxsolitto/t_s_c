{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425ceb85-d9ff-409a-880f-1d0f94c562c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import data\n",
    "from torchtext.legacy import data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e604666-0df1-4b16-9463-2d7f0f8d03c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading...\n",
      "Creating vocab...\n",
      "len(vocab)=646245\n",
      "Finish!!\n",
      "336.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2604"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vacab作成\n",
    "# テキストを単語で分割\n",
    "v_start = time.time()\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# data field定義\n",
    "TEXT  = data.Field(sequential=True,\n",
    "                     lower=True,\n",
    "                     batch_first=True, \n",
    "                     tokenize=tokenizer,\n",
    "                     init_token='<cls>')\n",
    "\n",
    "print(\"Reading...\")\n",
    "# CSVファイルを読み込み、TabularDatasetオブジェクトの作成\n",
    "vocab_data = data.TabularDataset(path ='tweet-transformer/1d/2021-17_t.csv',\n",
    "                                       format='csv',\n",
    "                                       skip_header = True,\n",
    "                                       fields=[('tweet', TEXT)])\n",
    "print(\"Creating vocab...\")\n",
    "\n",
    "# 単語辞書の作成\n",
    "TEXT.build_vocab(vocab_data, min_freq=3)\n",
    "vocab = TEXT.vocab\n",
    "print(f'{len(vocab)=}')\n",
    "\n",
    "print('Finish!!')\n",
    "print(f'{time.time() - v_start:5.2f} s')\n",
    "\n",
    "# メモリ開放\n",
    "del v_start, vocab_data, tokenizer, TEXT\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9406e05f-80e9-4f74-92d8-409d156844a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset1の定義\n",
    "class CreateDataset1(Dataset):\n",
    "    def __init__(self, x, y, tokenizer, vocab, max_len):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # len(Dataset)で返す値を指定    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    # Dataset[index]で返す値を指定\n",
    "    def __getitem__(self, index):\n",
    "        text = self.x[index]\n",
    "        text = self.tokenizer(text)\n",
    "        ids  = torch.tensor([self.vocab[word] for word in text], dtype=torch.long) # [seq_len]\n",
    "        ids  = F.pad(ids, (0 ,self.max_len-len(text)), \"constant\", 0) # [max_len]\n",
    "        mask = (ids==0)\n",
    "\n",
    "        section = self.y[index]\n",
    "\n",
    "        return {'ids'   : ids,\n",
    "                'mask'  : mask,\n",
    "                'section': torch.Tensor([section])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9d930b-c4e0-497b-838e-32b59adabb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset1をセクションごとにリスト分割\n",
    "def separate_section(dataset):\n",
    "    ids_list  = []\n",
    "    mask_list = []\n",
    "    start_time = time.time()\n",
    "    counter = -1\n",
    "    \n",
    "    for k in range(0, len(dataset)):\n",
    "        i = dataset[k]['ids'].squeeze()\n",
    "        m = dataset[k]['mask'].squeeze()\n",
    "            \n",
    "        if counter != dataset[k]['section']:\n",
    "            ids_list.append([i])\n",
    "            mask_list.append([m])\n",
    "            counter += 1\n",
    "        else:\n",
    "            ids_list[counter].append(i)\n",
    "            mask_list[counter].append(m)\n",
    "            \n",
    "        if k%1000000==0 and k!=0:\n",
    "            print(f'| 現在 {k:8d}件 終了 | 経過時間 {time.time()-start_time:6.2f} s |')\n",
    "    \n",
    "    print(f'{len(ids_list)=}')\n",
    "    print(f'{len(mask_list)=}')\n",
    "    \n",
    "    del i, m, k, start_time, counter\n",
    "    gc.collect()\n",
    "    \n",
    "    return ids_list, mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70cac4e7-4cf8-4513-9c23-d5d2481c3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset2の定義\n",
    "class CreateDataset2(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    # len(Dataset)で返す値を指定\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    # Dataset[index]で返す値を指定\n",
    "    def __getitem__(self, index):\n",
    "        ids  = self.x[index]\n",
    "        mask = self.y[index]\n",
    "\n",
    "        return {'ids'   : ids,\n",
    "                'mask'  : mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d47a3777-0423-4ea8-a3bd-7fd9421abb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetの作成 (ツイート)\n",
    "# 1. CreateDataset1\n",
    "# 2. separate_section\n",
    "# 3. CreateDataset2\n",
    "def data_process1(vocab,timespan):\n",
    "    max_len = 128\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "    print('Reading...')\n",
    "    df = pd.read_csv(f'tweet-transformer/{timespan}/2021-17_t.csv')\n",
    "    df = df.dropna(how='any')\n",
    "    df = df.reset_index(drop=True)\n",
    "    print('Creating Dataset1...')\n",
    "    dataset = CreateDataset1(df['tweet(n)'],  df['section'],  tokenizer, vocab, max_len)\n",
    "    print('Separating Section...')\n",
    "    ids_list, mask_list = separate_section(dataset)\n",
    "    \n",
    "    print('Creating Dataset2...')\n",
    "    dataset_tlist = []\n",
    "    for i in range(0, len(ids_list)):\n",
    "        x = CreateDataset2(ids_list[i], mask_list[i])\n",
    "        dataset_tlist.append(x)\n",
    "    \n",
    "    print('Finish!!')\n",
    "    print(f'{len(dataset_tlist)=}')\n",
    "    \n",
    "    del max_len, tokenizer, df, ids_list, mask_list, dataset, x, i\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataset_tlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec91bd8-e6fd-4656-9c2a-b1883d01483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset3の定義\n",
    "class CreateDataset3(Dataset):\n",
    "    def __init__(self, x, y, z):\n",
    "        self.x = x # tensor of section\n",
    "        self.y = y # tensor of price\n",
    "        self.z = z # tensor of trend(n+1)\n",
    "        \n",
    "    # len(Dataset)で返す値を指定\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    # Dataset[index]で返す値を指定\n",
    "    def __getitem__(self, index):\n",
    "        section = self.x[index]\n",
    "        src     = self.y[index]\n",
    "        target  = self.z[index]\n",
    "\n",
    "        return {'section': section,\n",
    "                'src'    : src,\n",
    "                'target' : target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da5a352-fff4-4227-8770-91df5e6f9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetの作成 (価格)\n",
    "# csv読み込み　特徴量n区間文挿入　欠損値いったん削除　 欠損区間を-1で埋める 3つのテンソル化　CreateDataset3に渡す\n",
    "def data_process2(timespan,n):\n",
    "\n",
    "    print('Reading...')\n",
    "    df  = pd.read_csv(f'tweet-transformer/{timespan}/2021-17_b.csv')\n",
    "    dfs = pd.read_csv(f'tweet-transformer/{timespan}/2021-17_s.csv')\n",
    "    \n",
    "    # 説明変数、目的変数\n",
    "    df['trend(n+1)'] = df['trend(n)'].shift(-1)\n",
    "    df['end_price(n)'] = df['open_price(n)'].shift(-1)\n",
    "    if n >= 2:\n",
    "        for i in range(1,n):\n",
    "            df[f'trend(n-{i})'] = df['trend(n)'].shift(i)\n",
    "            df[f'end_price(n-{i})'] = df['end_price(n)'].shift(i)\n",
    "    df = df.drop(columns=['open_price(n)'])\n",
    "    df = df.dropna(how='any')\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # sectionのみのデータフレームとマージ\n",
    "    # 欠損値がある行のSectionを-１に置き換え\n",
    "    # 欠損値をすべて-1に置き換え\n",
    "    \n",
    "    \n",
    "    pd.merge(dfs, df, on='section')\n",
    "    df = df.fillna(-1)\n",
    "    \n",
    "    section = torch.tensor(df['section'].values)\n",
    "    price   = torch.tensor(df.drop(columns=['trend(n+1)','section']).values)\n",
    "    target  = torch.tensor(df['trend(n+1)'].values)\n",
    "    print('Creating Dataset3...')\n",
    "    dataset_plist = CreateDataset3(section, price, target)\n",
    "    \n",
    "    print('Finish!!')\n",
    "    print(f'{len(dataset_list)=}')\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    return dataset_plist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39847b30-de3d-497b-b623-fbac89fb8ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f7afac-54ab-4475-b276-d3b7cd2fa0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametator for Net\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "d_model = 512  # embedding dimension\n",
    "nhead   = 8    # number of heads in nn.MultiheadAttention\n",
    "d_hid   = 2048  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 6    # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "dropout = 0.2  # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4024d1bf-ade2-4b1a-987c-e759fb32d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerモデルの概要\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 ntoken: int,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 d_hid: int,\n",
    "                 nlayers: int,\n",
    "                 dropout: float = 0.5,\n",
    "                 lstm_input_dim: int, \n",
    "                 lstm_hidden_dim: int,):\n",
    "\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(ntoken,\n",
    "                                d_model,\n",
    "                                padding_idx=0)\n",
    "        self.pos_encoder = PositionalEncoding(d_model,\n",
    "                                              dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model,\n",
    "                                                 nhead,\n",
    "                                                 d_hid,\n",
    "                                                 dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers,\n",
    "                                                      nlayers)\n",
    "        self.dense1 = nn.Linear(d_model,3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.input_dim = lstm_input_dim\n",
    "        self.hidden_dim = lstm_hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=lstm_input_dim, \n",
    "                            hidden_size=lstm_hidden_dim,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True)\n",
    "        self.dense2 = nn.Linear(lstm_hidden_dim,3)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.dense.bias.data.zero_()\n",
    "        self.dense.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    #データの流れ\n",
    "    def forward(self, train_tlist_b,train_plist_b ):\n",
    "        '''\n",
    "        Args:\n",
    "            src: Tensor, shape [batch_size, seq_len]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [batch_size, nclass:3]\n",
    "        '''\n",
    "        # Transformerによるテキストの3値分類\n",
    "        p_section = []\n",
    "        # out : list of tesnsor[neg,neu,pos]\n",
    "        for k in range(0,len(train_tlist_b)):\n",
    "            p_tbatch = []\n",
    "            tbatches = DataLoader(train_tlist_b[k], batch_size=1024, shuffle=True)\n",
    "\n",
    "            # out : list of tensor[batch_size,3]\n",
    "            for batch in tbatches:\n",
    "                ids  =  bacth['ids'].to(device)\n",
    "                mask =  batch['mask'].to(device)\n",
    "                x = self.embedding(ids) * math.sqrt(self.d_model)\n",
    "                print(1, x.size())\n",
    "                x = self.pos_encoder(x)\n",
    "                print(2, x.size())\n",
    "                x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "                print(3, x.size())\n",
    "                x = x.mean(dim=1)\n",
    "                print(4, x.size())\n",
    "                x = self.dense1(x)\n",
    "                print(5, x.size())\n",
    "                x = self.softmax(x)\n",
    "                p_tbatch.append(x)\n",
    "\n",
    "            x = torch.cat(p_tbatch, dim=0)\n",
    "            print(6, x.size())\n",
    "            x = x.sum(dim=0)\n",
    "            print(7, x.size())\n",
    "            p_section.append(x)\n",
    "\n",
    "        #train_plist_b[j] のtensorと　p_sectionのtensorを結合\n",
    "        src    = []\n",
    "        target = []\n",
    "        for j in range(0, batch_size):\n",
    "            if train_plist_b[j]['section'] != -1:\n",
    "                x = torch.cat(p_section[j:j+n], dim=-1)\n",
    "                print(8, x.size())\n",
    "                x = torch.cat((x,train_plist_b[j]['src']), dim=-1)\n",
    "                print(9, x.size())\n",
    "                src.append(x)\n",
    "                src.append(train_plist_b[j]['target'])\n",
    "                \n",
    "        x = torch.cat(src, dim=0)\n",
    "        y = torch.cat(target, dim=-1)\n",
    "            \n",
    "            \n",
    "        # LSTMによるテキスト＋価格の３値分類\n",
    "        _, x = self.lstm(x)\n",
    "        x = self.dense2(x[0].view(inlist.size(0), -1))\n",
    "        #\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0419fd72-cc71-4ea6-9c03-14f8bc8118a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PositionalEncodingの概要\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        '''\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367e57a-678c-4d2e-bc15-5fdfd4df01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramator for training & evaluation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "softmax = nn.Softmax(dim=1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a77848dc-a80f-4233-a46d-82d327d1f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(model, train_tlist,train_plist, n, batch_size):\n",
    "    train_start_time = time.time()\n",
    "    model.train()\n",
    "    if (len(train_tlist)-n+1) % batch_size == 0:\n",
    "        num_batches = (len(train_tlist)-n+1) / batch_size\n",
    "    else:\n",
    "        num_batches = ((len(train_tlist)-n+1) // batch_size) + 1\n",
    "    log_interval = math.ceil(num_batches/30)*10\n",
    "    batch_counter = 0\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_count = 0\n",
    "    \n",
    "    \n",
    "    for i in range(0, num_batches):\n",
    "        train_tlist_b = train_tlist[i: i+batch_size+n-1]\n",
    "        \n",
    "        if i != (num_batches-1):\n",
    "            train_plist_b = train_plist[i*batch_size: (i+1)*batch_size]\n",
    "        else:\n",
    "            train_plist_b = train_plist[i*batch_size: len(train_plist)]\n",
    "            \n",
    "            \n",
    "        predictions = model()\n",
    "        prob = softmax(predictions)\n",
    "        labels = batch.label.to(device)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        correct = prob.argmax(axis=1) == labels\n",
    "        acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "        train_correct += correct.sum().item()\n",
    "        train_count += correct.size(0)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_counter += 1\n",
    "        \n",
    "        if batch_counter % log_interval == 0 or batch_counter == num_batches:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            s_per_batch = (time.time() - train_start_time) / log_interval\n",
    "            cur_loss = train_loss / log_interval\n",
    "            cur_acc = train_correct / train_count\n",
    "            print(f'| epoch {epoch:3d} | {batch_counter:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:1.5f} | s/batch {s_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | accuracy {cur_acc:8.2f}')\n",
    "            total_loss = 0\n",
    "            train_start_time = time.time()\n",
    "    \n",
    "    del \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f793e1-ba75-49cf-8fe6-3a497f39d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation (val, test)\n",
    "def evaluate(model, eval_list, n):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_correct = 0\n",
    "    eval_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(iter(eval_iter)):\n",
    "            predictions = model(batch.tweet_n.to(device))\n",
    "            prob = softmax(predictions)\n",
    "            labels = batch.label.to(device)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            correct = prob.argmax(axis=1) == labels\n",
    "            acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "            eval_correct += correct.sum().item()\n",
    "            eval_count += correct.size(0)\n",
    "            eval_loss += loss.item()\n",
    "        \n",
    "    print(f'| loss {eval_loss}| accuracy {eval_correct / ecal_count} ')\n",
    "        \n",
    "    return eval_loss, eval_correct / eval_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499797d0-d06b-4c55-8768-3def2b2ee0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "nlist = [1,2,3,4,5,6,7,8,9,10]\n",
    "tlist = ['1d','12h','4h','1h','30m','15m','5m']\n",
    "batch_size_list= {'1d':16,'12h':32,'4h':64,'1h':256,'30m':512,'15m':1024,'5m':2048}\n",
    "aculist = {}\n",
    "for timespan in tlist:\n",
    "    print(f'{timespan=}')\n",
    "    dataset_tlist = data_process1(vocab,timespan)\n",
    "    train_tlist, test_tlist = train_test_split(dataset_tlist, test_size = 1/7, shuffle=False)\n",
    "    print(f'{len(train_tlist)=}')\n",
    "    print(f'{len(test_tlist)=}')\n",
    "    batch_size=batch_size_list[timespan]\n",
    "    \n",
    "    del dataset_list\n",
    "    gc.collect()\n",
    "    \n",
    "    for n in nlist:\n",
    "        print(f'{n=}')\n",
    "\n",
    "        model = Net(ntokens, d_model, nhead, d_hid, nlayers, dropout).to(device)\n",
    "        lr = 1e-3\n",
    "        best_val_loss = float('inf')\n",
    "        epochs = 1\n",
    "        best_model = None\n",
    "\n",
    "        dt_start = datetime.datetime.now()\n",
    "        print(datetime.datetime.now())\n",
    "        print('***training start***')\n",
    "        print('-' * 95)\n",
    "\n",
    "        # training & test roop\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            train(model, train_tlist, n, batch_size)\n",
    "            val_loss, val_acc = evaluate(model, test_tlist, n)\n",
    "            print('-' * 95)\n",
    "            print(f'| end of epoch {epoch:3d} | time: {time.time()-epoch_start_time:5.2f}s | '\n",
    "                  f'val loss：{val_loss:5.3f} | val accuracy：{val_acc:8.3f}')\n",
    "            print('-' * 95)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            del epoch_start_time, val_loss, val_acc\n",
    "            gc.collect()\n",
    "\n",
    "        dt_end = datetime.datetime.now()\n",
    "        print(datetime.datetime.now())    \n",
    "        print(f'***Finish! training time：{dt_end - dt_start:5.2f}s***')\n",
    "        \n",
    "        # test\n",
    "        test_loss, test_acc = evaluate(best_model, test_list, n)\n",
    "        print('=' * 89)\n",
    "        print(f'| End of training | test loss：{test_loss:5.3f} | '\n",
    "              f'test accuracy：{test_acc:8.3f}')\n",
    "        print('=' * 89)\n",
    "        \n",
    "        del best_val_loss,epochs,model,best_model,test_loss,test_acc,dt_start,dt_end\n",
    "        gc.collect()\n",
    "        \n",
    "    del train_list, test_list\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9b279-ca38-4d79-a472-ca2529ca9bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58297e-125a-404c-b1ff-1808669dbed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ec3f1-3382-4c73-8e21-b436c35c5e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf691823-0891-4e28-87ea-82a5dff263ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26b8163e-c26d-4c9d-8b9f-ff6bb147d50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device0 = torch.device(\"cuda:0\")\n",
    "device1 = torch.device(\"cuda:1\")\n",
    "print(torch.cuda.is_available())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f659b64-a473-4361-a411-dfb2bb84d5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading...\n",
      "Creating Dataset1...\n",
      "Separating Section...\n",
      "| 現在  1000000件 終了 | 経過時間 121.63 s |\n",
      "| 現在  2000000件 終了 | 経過時間 243.59 s |\n",
      "| 現在  3000000件 終了 | 経過時間 362.32 s |\n",
      "| 現在  4000000件 終了 | 経過時間 487.47 s |\n",
      "| 現在  5000000件 終了 | 経過時間 610.35 s |\n",
      "| 現在  6000000件 終了 | 経過時間 730.27 s |\n",
      "| 現在  7000000件 終了 | 経過時間 854.38 s |\n",
      "| 現在  8000000件 終了 | 経過時間 978.32 s |\n",
      "| 現在  9000000件 終了 | 経過時間 1098.39 s |\n",
      "| 現在 10000000件 終了 | 経過時間 1224.66 s |\n",
      "| 現在 11000000件 終了 | 経過時間 1344.35 s |\n",
      "| 現在 12000000件 終了 | 経過時間 1472.51 s |\n",
      "| 現在 13000000件 終了 | 経過時間 1592.27 s |\n",
      "len(ids_list)=61056\n",
      "len(mask_list)=61056\n",
      "Creating Dataset2...\n",
      "Finish!!\n",
      "len(dataset_list)=61056\n"
     ]
    }
   ],
   "source": [
    "timespan='5m'\n",
    "dataset_list = data_process(vocab,timespan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d77282d6-4f55-48ab-9fab-7dfee079da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset_list)=61056\n",
      "len(dataset_list[0])=220\n",
      "len(dataset_list[1])=164\n",
      "len(dataset_list[2])=139\n",
      "len(dataset_list[1000])=196\n",
      "len(train_list)=52333\n",
      "len(test_list)=8723\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(dataset_list)=}')\n",
    "print(f'{len(dataset_list[0])=}')\n",
    "print(f'{len(dataset_list[1])=}')\n",
    "print(f'{len(dataset_list[2])=}')\n",
    "print(f'{len(dataset_list[1000])=}')\n",
    "\n",
    "train_list, test_list = train_test_split(dataset_list, test_size = 1/7, shuffle=False)\n",
    "print(f'{len(train_list)=}')\n",
    "print(f'{len(test_list)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec7e9c46-7cef-42a4-a8f4-bbc929297715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CreateDataset2 object at 0x7f08eef24d00>\n",
      "tensor([[  952,    92,  1622,  ...,     0,     0,     0],\n",
      "        [  231, 25997,   595,  ...,     0,     0,     0],\n",
      "        [    6,  7765,   562,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  116,   350,    81,  ...,     0,     0,     0],\n",
      "        [ 1099,   134,     6,  ...,     0,     0,     0],\n",
      "        [  126,     5,    37,  ...,     0,     0,     0]])\n",
      "tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]])\n",
      "torch.Size([220, 128])\n",
      "torch.Size([220, 128])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_list[0])\n",
    "dataloader_train = DataLoader(train_list[0], batch_size=1024, shuffle=True)\n",
    "c=0\n",
    "for data in dataloader_train:\n",
    "    print(data['ids'])\n",
    "    print(data['mask'])\n",
    "    print(data['ids'].size())\n",
    "    print(data['mask'].size())\n",
    "    print('\\n')\n",
    "    c+=1\n",
    "    if c==5:break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da5ffd4b-317b-464e-b0e1-9f0f6be9f5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4]\n"
     ]
    }
   ],
   "source": [
    "l=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "i=3\n",
    "n=2\n",
    "batch_size=4\n",
    "num_batches=6\n",
    "tl = l[i: i+n]\n",
    "print(tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90edfab9-62a7-4b99-9907-be52ae84c1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[ 0.0648,  0.2500, -0.3278],\n",
      "        [ 0.7434,  0.5512, -0.1645]])\n",
      "tensor([[ 0.0648,  0.2500, -0.3278]]) tensor([[ 0.7434,  0.5512, -0.1645]]) tensor([[-0.1361,  0.5010,  0.1875]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "n=2\n",
    "x1=torch.randn(3).unsqueeze(0)\n",
    "x2=torch.randn(3).unsqueeze(0)\n",
    "x3=torch.randn(3).unsqueeze(0)\n",
    "x=torch.tensor([[1,1,1]])\n",
    "l=[x1,x2,x3]\n",
    "out = torch.cat(l[0:0+n], dim=0)\n",
    "print(out.size())\n",
    "print(out)\n",
    "print(x1,x2,x3)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53e4f516-de97-44c7-a2f1-968d23ad6348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading...\n",
      "     section  trend(n)  trend(n+1)  end_price(n)  trend(n-1)  end_price(n-1)  \\\n",
      "199      201         0         2.0      32144.51         0.0        29790.34   \n",
      "200      202         2         1.0      32287.58         0.0        32144.51   \n",
      "201      203         1         2.0      33634.10         2.0        32287.58   \n",
      "202      204         2         2.0      34261.51         1.0        33634.10   \n",
      "203      205         2         2.0      35381.02         2.0        34261.51   \n",
      "204      206         2         2.0      37241.33         2.0        35381.02   \n",
      "205      207         2         2.0      39456.61         2.0        37241.33   \n",
      "206      208         2         1.0      40019.57         2.0        39456.61   \n",
      "207      209         1         1.0      40018.49         2.0        40019.57   \n",
      "208      210         1         2.0      42206.36         1.0        40018.49   \n",
      "\n",
      "     trend(n-2)  end_price(n-2)  \n",
      "199         1.0        30839.65  \n",
      "200         0.0        29790.34  \n",
      "201         0.0        32144.51  \n",
      "202         2.0        32287.58  \n",
      "203         1.0        33634.10  \n",
      "204         2.0        34261.51  \n",
      "205         2.0        35381.02  \n",
      "206         2.0        37241.33  \n",
      "207         2.0        39456.61  \n",
      "208         2.0        40019.57  \n",
      "209\n"
     ]
    }
   ],
   "source": [
    "print('Reading...')\n",
    "timespan = '1d'\n",
    "n=3\n",
    "df = pd.read_csv(f'tweet-transformer/{timespan}/2021-17_b.csv')\n",
    "\n",
    "df['trend(n+1)'] = df['trend(n)'].shift(-1)\n",
    "df['end_price(n)'] = df['open_price(n)'].shift(-1)\n",
    "if n >= 2:\n",
    "    for i in range(1,n):\n",
    "        df[f'trend(n-{i})'] = df['trend(n)'].shift(i)\n",
    "        df[f'end_price(n-{i})'] = df['end_price(n)'].shift(i)\n",
    "df = df.drop(columns=['open_price(n)'])\n",
    "df = df.dropna(how='any')\n",
    "df = df.reset_index(drop=True)\n",
    "print(df.tail(10))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf1d3d71-24ea-4471-a750-1f4be2af8e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([209]) tensor([2., 0., 2., 2., 2., 2., 1., 0., 0., 0., 2., 2., 0., 0., 1., 2., 0., 1.,\n",
      "        0., 2., 0., 1., 1., 1., 0., 2., 2., 1., 1., 1., 2., 2., 0., 2., 2., 1.,\n",
      "        2., 1., 0., 2., 1., 1., 2., 1., 2., 2., 1., 2., 1., 2., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 2., 0., 2., 0., 1., 1., 2., 2., 2., 2., 2., 1., 2., 0., 0.,\n",
      "        2., 2., 0., 1., 1., 1., 0., 1., 0., 0., 2., 1., 1., 2., 2., 1., 1., 1.,\n",
      "        0., 2., 1., 0., 0., 2., 1., 2., 1., 1., 2., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 2., 2., 1., 0., 1., 1., 0., 1., 0., 2., 0., 1., 2.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 2., 0., 1., 0., 2., 1., 2.,\n",
      "        0., 0., 0., 2., 1., 1., 2., 2., 0., 0., 1., 0., 1., 2., 0., 2., 0., 2.,\n",
      "        2., 1., 0., 1., 0., 1., 1., 0., 2., 2., 2., 0., 2., 2., 1., 2., 1., 0.,\n",
      "        1., 2., 2., 0., 1., 1., 0., 2., 1., 2., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 2., 1., 2., 2., 2., 2., 2., 1., 1., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "label = torch.tensor(df.drop(columns=['trend(n+1)','section']).values)\n",
    "label = torch.tensor(df['trend(n+1)'].values)\n",
    "print(label.size(), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d6cdaa87-8a6a-42cf-961e-2418e2fe85d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n",
      "{'ids': tensor(2., dtype=torch.float64), 'mask': tensor(2., dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "x=CreateDataset2(label,label)\n",
    "print(len(x))\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ddd248-0748-4bec-b90a-c06b89449684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
