{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425ceb85-d9ff-409a-880f-1d0f94c562c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import data\n",
    "from torchtext.legacy import data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import datetime\n",
    "import shutil\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08f132e-a961-4c15-bc82-e95d150200e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle書き込み\n",
    "def write_pickle(filepath, data):\n",
    "    start_time = time.time()\n",
    "    print(f'writing pickle to \"{filepath}\" ...')    \n",
    "    \n",
    "    with open(filepath, 'wb') as p:\n",
    "        pickle.dump(data,p)\n",
    "    \n",
    "    print(f'end of writeing {time.time()-start_time:6.2f} s')\n",
    "    \n",
    "    del start_time\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a06a96-7d3e-463b-99fd-1af31e09e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle書き込み\n",
    "# ログなしVer\n",
    "def write_pickle_quickly(filepath, data):\n",
    "    with open(filepath, 'wb') as p:\n",
    "        pickle.dump(data,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2ce5412-6421-4e23-931c-e7fc004794ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle読み出し\n",
    "def read_pickle(filepath):\n",
    "    start_time = time.time()\n",
    "    print(f'reading pickle from \"{filepath}\" ...')\n",
    "    \n",
    "    with open(filepath, 'rb') as p:\n",
    "        data = pickle.load(p)\n",
    "    \n",
    "    print(f'end of reading {time.time()-start_time:6.2f} s')\n",
    "    \n",
    "    del start_time\n",
    "    gc.collect()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394b3a48-81b7-4252-8515-ef4da43d5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle読み出し\n",
    "# ログなしVer\n",
    "def read_pickle_quickly(filepath):\n",
    "    with open(filepath, 'rb') as p:\n",
    "        data = pickle.load(p)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e604666-0df1-4b16-9463-2d7f0f8d03c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading...\n",
      "reading pickle from \"../../external_drive/pickle/vocab.pickle\" ...\n",
      "end of reading   1.98 s\n",
      "Finish!!\n",
      " 2.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nv_start = time.time()\\ntokenizer = get_tokenizer(\\'basic_english\\')\\n\\n# data field定義\\nTEXT  = data.Field(sequential=True,\\n                     lower=True,\\n                     batch_first=True, \\n                     tokenize=tokenizer,\\n                     init_token=\\'<cls>\\')\\n\\n# CSVファイルを読み込み、TabularDatasetオブジェクトの作成\\nprint(\"Reading...\")\\nvocab_data = data.TabularDataset(path =\\'tweet-transformer/1d/2021-17_t.csv\\',\\n                                       format=\\'csv\\',\\n                                       skip_header = True,\\n                                       fields=[(\\'tweet\\', TEXT)])\\n\\n# 単語辞書の作成\\nprint(\"Creating vocab...\")\\nTEXT.build_vocab(vocab_data, min_freq=3)\\nvocab = TEXT.vocab\\nprint(f\\'{len(vocab)=}\\')\\n\\nprint(\\'Finish!!\\')\\nprint(f\\'{time.time() - v_start:5.2f} s\\')\\n\\n# メモリ開放\\ndel v_start, vocab_data, tokenizer, TEXT\\ngc.collect()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vacab作成\n",
    "# テキストを単語で分割\n",
    "\n",
    "v_start = time.time()\n",
    "print(\"Reading...\")\n",
    "vocab = read_pickle('../../external_drive/pickle/vocab.pickle')\n",
    "print('Finish!!')\n",
    "print(f'{time.time() - v_start:5.2f} s')\n",
    "del v_start\n",
    "gc.collect()\n",
    "\n",
    "'''\n",
    "v_start = time.time()\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# data field定義\n",
    "TEXT  = data.Field(sequential=True,\n",
    "                     lower=True,\n",
    "                     batch_first=True, \n",
    "                     tokenize=tokenizer,\n",
    "                     init_token='<cls>')\n",
    "\n",
    "# CSVファイルを読み込み、TabularDatasetオブジェクトの作成\n",
    "print(\"Reading...\")\n",
    "vocab_data = data.TabularDataset(path ='tweet-transformer/1d/2021-17_t.csv',\n",
    "                                       format='csv',\n",
    "                                       skip_header = True,\n",
    "                                       fields=[('tweet', TEXT)])\n",
    "\n",
    "# 単語辞書の作成\n",
    "print(\"Creating vocab...\")\n",
    "TEXT.build_vocab(vocab_data, min_freq=3)\n",
    "vocab = TEXT.vocab\n",
    "print(f'{len(vocab)=}')\n",
    "\n",
    "print('Finish!!')\n",
    "print(f'{time.time() - v_start:5.2f} s')\n",
    "\n",
    "# メモリ開放\n",
    "del v_start, vocab_data, tokenizer, TEXT\n",
    "gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9406e05f-80e9-4f74-92d8-409d156844a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset1の定義\n",
    "# args　：tdf['ids'], tdf['mask']\n",
    "# return：dataset{ids,mask}\n",
    "\n",
    "class CreateDataset1(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x # tdf['ids']\n",
    "        self.y = y # tdf['mask']\n",
    "        \n",
    "    # len(Dataset)で返す値を指定\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    # Dataset[index]で返す値を指定\n",
    "    def __getitem__(self, index):\n",
    "        ids  = self.x[index]\n",
    "        mask = self.y[index]\n",
    "\n",
    "        return {'ids'   : ids,\n",
    "                'mask'  : mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9d930b-c4e0-497b-838e-32b59adabb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset3の定義\n",
    "# args　：tensor of section, tensor of price, tensor of trend(n+1)\n",
    "# return：dataset3{section, src, target}\n",
    "class CreateDataset3(Dataset):\n",
    "    def __init__(self, x, y, z):\n",
    "        self.x = x # tensor of section\n",
    "        self.y = y # tensor of price\n",
    "        self.z = z # tensor of trend(n+1)\n",
    "        \n",
    "    # len(Dataset)で返す値を指定\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    # Dataset[index]で返す値を指定\n",
    "    def __getitem__(self, index):\n",
    "        section = self.x[index]\n",
    "        src     = self.y[index]\n",
    "        target  = self.z[index]\n",
    "\n",
    "        return {'section': section,\n",
    "                'src'    : src,\n",
    "                'target' : target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70cac4e7-4cf8-4513-9c23-d5d2481c3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_df()内の関数\n",
    "max_len = 128\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "def tokenize(text):\n",
    "    return tokenizer(text)\n",
    "\n",
    "def text_to_ids(tokenized_text):\n",
    "    ids  = torch.tensor([vocab[word] for word in tokenized_text], dtype=torch.long).unsqueeze(0) # [1,seq_len]\n",
    "    ids  = F.pad(ids, (0 ,max_len-len(tokenized_text)), \"constant\", 0) # [1,max_len]\n",
    "    return ids\n",
    "\n",
    "def ids_to_mask(ids):\n",
    "    mask = (ids==0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d47a3777-0423-4ea8-a3bd-7fd9421abb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return df[section.ids,mask]\n",
    "def tensor_df(df):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('1.text to ids...')\n",
    "    df['tweet(n)'] = df['tweet(n)'].apply(tokenize)\n",
    "    df['ids']      = df['tweet(n)'].apply(text_to_ids)\n",
    "    print('2.ids to mask...')\n",
    "    df['mask']     = df['ids'].apply(ids_to_mask)\n",
    "    df = df.drop(columns=['tweet(n)'])\n",
    "    \n",
    "    print(f'end of df to tensor {time.time()-start_time:6.2f} s')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec91bd8-e6fd-4656-9c2a-b1883d01483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return : df[section,ids(n),mask(n)]\n",
    "# sectionごとにtensor連結\n",
    "train_num_section_list = {'1d':181,'12h':363,'4h':1090,'1h':4361,'30m':8722,'15m':17444,'5m':52333}\n",
    "test_num_section_list  = {'1d':31, '12h':61, '4h':182, '1h':727, '30m':1454,'15m':2908, '5m':8723}\n",
    "\n",
    "def separate_section(df):\n",
    "    num_section = train_num_section_list[timespan] + test_num_section_list[timespan]\n",
    "    section_list = []\n",
    "    ids_list  = []\n",
    "    mask_list = []\n",
    "    for i in range (0,num_section):\n",
    "        #pandasのSectionkを抽出　idsとmaskをひとまとまりのtensorに\n",
    "        section_list.append(i)\n",
    "        df_ids = df[df['section'] == i]['ids']\n",
    "        l = df_ids.values.tolist()\n",
    "        x = torch.cat(l, dim=0)\n",
    "        ids_list.append(x)\n",
    "        df_mask = df[df['section'] == i]['mask']\n",
    "        l = df_mask.values.tolist()\n",
    "        x = torch.cat(l, dim=0)\n",
    "        mask_list.append(x)\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(section_list, ids_list, mask_list)), columns = ['section','ids','mask'])\n",
    "    \n",
    "    return df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2da5a352-fff4-4227-8770-91df5e6f9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetの作成 (ツイート)\n",
    "# 1. df to tensor\n",
    "# 2. separate_section\n",
    "def data_process1(timespan):\n",
    "    print('-'*5 + 'Create dataset_tlist start!!' + '-'*5)\n",
    "\n",
    "    print('Reading...')\n",
    "    dfs = pd.read_csv(f'tweet-transformer/{timespan}/2021-17_s.csv')\n",
    "    df = pd.read_csv(f'tweet-transformer/{timespan}/2021-17_t.csv')\n",
    "    df = df.dropna(how='any')\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    print('df to tensor...')\n",
    "    df = tensor_df(df)\n",
    "    \n",
    "    print('Separating Section...')\n",
    "    df = separate_section(df)\n",
    "        \n",
    "    train_tdf, test_tdf = train_test_split(df, test_size = 1/7, shuffle=False)\n",
    "    \n",
    "    print('Finish!!')\n",
    "    print(f'{len(train_tdf)=}')\n",
    "    print(f'{len(test_tdf)=}')\n",
    "    \n",
    "    return train_tdf, test_tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31b9b279-ca38-4d79-a472-ca2529ca9bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetの作成 (価格)\n",
    "# 1.csv -> 3 tensor\n",
    "# 2.CreateDataset3\n",
    "def data_process2(timespan,n):\n",
    "    print('-'*5 + 'Create dataset_plist start!!' + '-'*5)\n",
    "\n",
    "    print('Reading...')\n",
    "    df  = pd.read_csv(f'tweet-transformer/{timespan}/2021-17_b.csv')\n",
    "    dfs = pd.read_csv(f'tweet-transformer/{timespan}/2021-17_s.csv')\n",
    "\n",
    "    # 説明変数、目的変数\n",
    "    df['trend(n+1)'] = df['trend(n)'].shift(-1)\n",
    "    df['end_price(n)'] = df['open_price(n)'].shift(-1)\n",
    "    if n >= 2:\n",
    "        for i in range(1,n):\n",
    "            df[f'trend(n-{i})'] = df['trend(n)'].shift(i)\n",
    "            df[f'end_price(n-{i})'] = df['end_price(n)'].shift(i)\n",
    "    df = df.drop(columns=['open_price(n)'])\n",
    "    df = df.dropna(how='any')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # マージして欠損値を含む行を処理\n",
    "    df = pd.merge(dfs, df, on=\"section\", how = 'left')\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['bool'] = df.isnull().any(axis=1)\n",
    "    for i in range(0, len(df)):\n",
    "        if  df['bool'][i] == True:\n",
    "            df['section'][i] = -1\n",
    "    df = df.fillna(-1)\n",
    "    df = df.drop(columns=['bool'])\n",
    "    print(f'{len(df)=}')\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size = 1/7, shuffle=False)\n",
    "\n",
    "    # 3つのtensorを作成\n",
    "    print('Creating Dataset3...')\n",
    "    section = torch.tensor(train_df['section'].values)\n",
    "    target  = torch.tensor(train_df['trend(n+1)'].values)\n",
    "    price   = torch.tensor(train_df.drop(columns=['trend(n+1)','section']).values)\n",
    "    train_plist = CreateDataset3(section, price, target)\n",
    "\n",
    "    section = torch.tensor(test_df['section'].values)\n",
    "    target  = torch.tensor(test_df['trend(n+1)'].values)\n",
    "    price   = torch.tensor(test_df.drop(columns=['trend(n+1)','section']).values)\n",
    "    test_plist = CreateDataset3(section, price, target)\n",
    "\n",
    "\n",
    "    print('Finish!!')\n",
    "    print(f'{len(train_plist)=}')\n",
    "    print(f'{len(test_plist)=}')\n",
    "    \n",
    "    del df,dfs,section,price,target,train_df,test_df\n",
    "    gc.collect()\n",
    "    \n",
    "    return train_plist, test_plist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec5d0ef2-ffc7-4313-9c8c-f32b0cfacc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntlist = ['1d','12h','4h','1h','30m','15m','5m']\\ntrain_section_list     = {'1d':181,'12h':363,'4h':1090,'1h':4361,'30m':8722,'15m':17444,'5m':52333}\\ntest_section_list      = {'1d':31, '12h':61, '4h':182, '1h':727, '30m':1454,'15m':2908, '5m':8723}\\n\\nfor timespan in tlist:\\n    print('-'*50 + f'{timespan=}' + '-'*50)\\n    train_tdf, test_tdf = data_process1(timespan)\\n    print(f'{sys.getsizeof(train_tdf)=}')\\n    print(f'{sys.getsizeof(test_tdf)=}')\\n    train_tdf.to_csv(f'../../external_drive/pandas/{timespan}/train_tdf.csv', index=False)\\n    test_tdf.to_csv(f'../../external_drive/pandas/{timespan}/test_tdf.csv', index=False) \\n    write_pickle(f'../../external_drive/pickle/{timespan}/train_tdf.pickle',train_tdf)\\n    write_pickle(f'../../external_drive/pickle/{timespan}/test_tdf.pickle',test_tdf)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データセット作成 & csv, pickleに保存\n",
    "# ツイート\n",
    "'''\n",
    "tlist = ['1d','12h','4h','1h','30m','15m','5m']\n",
    "train_section_list     = {'1d':181,'12h':363,'4h':1090,'1h':4361,'30m':8722,'15m':17444,'5m':52333}\n",
    "test_section_list      = {'1d':31, '12h':61, '4h':182, '1h':727, '30m':1454,'15m':2908, '5m':8723}\n",
    "\n",
    "for timespan in tlist:\n",
    "    print('-'*50 + f'{timespan=}' + '-'*50)\n",
    "    train_tdf, test_tdf = data_process1(timespan)\n",
    "    print(f'{sys.getsizeof(train_tdf)=}')\n",
    "    print(f'{sys.getsizeof(test_tdf)=}')\n",
    "    train_tdf.to_csv(f'../../external_drive/pandas/{timespan}/train_tdf.csv', index=False)\n",
    "    test_tdf.to_csv(f'../../external_drive/pandas/{timespan}/test_tdf.csv', index=False) \n",
    "    write_pickle(f'../../external_drive/pickle/{timespan}/train_tdf.pickle',train_tdf)\n",
    "    write_pickle(f'../../external_drive/pickle/{timespan}/test_tdf.pickle',test_tdf)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882a077a-81fe-4d3d-a879-ddcd3fea01d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnlist = [1,2,3,4,5,6,7,8,9,10]\\ntlist = ['1d','12h','4h','1h','30m','15m','5m']\\n\\nfor timespan, n in itertools.product(tlist, nlist):   \\n\\n    print('-'*50 + f'{timespan=} / {n=}' + '-'*50)\\n    train_plist,test_plist = data_process2(timespan,n)\\n    write_pickle(f'../../external_drive/pickle/{timespan}/train_plist_{n}.pickle',train_plist)\\n    write_pickle(f'../../external_drive/pickle/{timespan}/test_plist_{n}.pickle',test_plist)\\n    \\n    del train_plist, test_plist\\n    gc.collect()\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データセット作成 & pickleに保存\n",
    "# 価格\n",
    "'''\n",
    "nlist = [1,2,3,4,5,6,7,8,9,10]\n",
    "tlist = ['1d','12h','4h','1h','30m','15m','5m']\n",
    "\n",
    "for timespan, n in itertools.product(tlist, nlist):   \n",
    "\n",
    "    print('-'*50 + f'{timespan=} / {n=}' + '-'*50)\n",
    "    train_plist,test_plist = data_process2(timespan,n)\n",
    "    write_pickle(f'../../external_drive/pickle/{timespan}/train_plist_{n}.pickle',train_plist)\n",
    "    write_pickle(f'../../external_drive/pickle/{timespan}/test_plist_{n}.pickle',test_plist)\n",
    "    \n",
    "    del train_plist, test_plist\n",
    "    gc.collect()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f7afac-54ab-4475-b276-d3b7cd2fa0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametator for Net\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "d_model = 512   # embedding dimension\n",
    "nhead   = 8     # number of heads in nn.MultiheadAttention\n",
    "d_hid   = 2048  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 6     # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "dropout = 0.2   # dropout probability\n",
    "lstm_input_dim  = 5\n",
    "lstm_hidden_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4024d1bf-ade2-4b1a-987c-e759fb32d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer-LSTMモデルの概要\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 ntoken: int,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 d_hid: int,\n",
    "                 nlayers: int,\n",
    "                 lstm_input_dim: int, \n",
    "                 lstm_hidden_dim: int,\n",
    "                 dropout: float = 0.5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(ntoken,\n",
    "                                d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model,\n",
    "                                              dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model,\n",
    "                                                 nhead,\n",
    "                                                 d_hid,\n",
    "                                                 dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers,\n",
    "                                                      nlayers)\n",
    "        self.dense1 = nn.Linear(d_model,3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.input_dim = lstm_input_dim\n",
    "        self.hidden_dim = lstm_hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=lstm_input_dim, \n",
    "                            hidden_size=lstm_hidden_dim,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True)\n",
    "        self.dense2 = nn.Linear(lstm_hidden_dim,3)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.dense1.bias.data.zero_()\n",
    "        self.dense1.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    #データの流れ\n",
    "    def forward(self,timespan, n, t_start, t_end, tdf, plist, train_test_flag):\n",
    "\n",
    "        tbatch_size_list = {'4h':512, '30m':256, '5m':64}\n",
    "        tbatch_size = tbatch_size_list[timespan]\n",
    "        if timespan == '5m':\n",
    "            r = 30\n",
    "        elif timespan == '30m':\n",
    "            r = 40\n",
    "        else:\n",
    "            r = 60\n",
    "        idx_counter_list=[]\n",
    "        \n",
    "        if train_test_flag == 1:\n",
    "            t_start += train_num_section \n",
    "            t_end  += train_num_section\n",
    "            \n",
    "        for j in range(t_start,t_end):   \n",
    "            tlist = CreateDataset1(tdf['ids'][j], tdf['mask'][j])\n",
    "            tbatches = iter(DataLoader(tlist, batch_size=tbatch_size_list[timespan], shuffle=True))\n",
    "            idx_counter = 0\n",
    "\n",
    "            for idx,batch in enumerate(tbatches): \n",
    "                if idx % r == 0:\n",
    "                    idx_counter += 1\n",
    "                    ids  =  batch['ids'].to(device)  # [batch_size, seq_len]\n",
    "                    mask =  torch.t(batch['mask']).to(device) # [seq_len, batch_size]\n",
    "                    \n",
    "                    # Transformerによるテキストの3値分類           \n",
    "                    x = self.embedding(ids) * math.sqrt(self.d_model) # [batch_size, seq_len, d_model]\n",
    "                    x = self.pos_encoder(x) # [batch_size, seq_len, d_model]\n",
    "                    x = self.transformer_encoder(src=x, src_key_padding_mask=mask) # [batch_size, seq_len, d_model]\n",
    "                    x[x != x] = 0 #Nanを0に置き換え\n",
    "                    x = x.mean(dim=1) # [batch_size, d_model]\n",
    "                    x = self.dense1(x) # [batch_size, 3]\n",
    "                    \n",
    "                    x = softmax(x) # [batch_size, 3] x=prob\n",
    "                    write_pickle_quickly(f'../../external_drive/pickle/temp/{j}_{idx}.pickle',x.to('cpu'))\n",
    "                    del ids, mask, x\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "            idx_counter_list.append(idx_counter)\n",
    "\n",
    "        prob_section = []\n",
    "        for j in range(t_start,t_end):\n",
    "            p_tbatch = []\n",
    "            for idx in range(0, idx_counter_list[j-t_start]):\n",
    "                prob = read_pickle_quickly(f'../../external_drive/pickle/temp/{j}_{idx*r}.pickle') # [batch_size, 3]\n",
    "                p_tbatch.append(prob)\n",
    "                os.remove(f'../../external_drive/pickle/temp/{j}_{idx*r}.pickle')\n",
    "            x = torch.cat(p_tbatch, dim=0) # [batch_size*num_tbatches, 3]\n",
    "            x = x.sum(dim=0) # [3]\n",
    "            prob_section.append(x)\n",
    "\n",
    "        src    = []\n",
    "        target = []\n",
    "        \n",
    "        if train_test_flag == 1:\n",
    "            t_start -= train_num_section \n",
    "            t_end  -= train_num_section\n",
    "\n",
    "        for j in range(t_start+n-1,t_end):\n",
    "            if plist[j]['section'] != -1:\n",
    "                l=[]\n",
    "                x = plist[j]['src'] #[2n]\n",
    "                y = torch.cat(prob_section[j-(t_start+n-1): j-(t_start+n-1)+n], dim=-1) # [3n]\n",
    "                for k in range (0,n):   \n",
    "                    z = torch.cat((x[k*2:(k+1)*2], y[3*n-(k+1)*3:3*n-k*3]), dim=-1).unsqueeze(0) # [1,5]\n",
    "                    l.append(z)\n",
    "                z = torch.cat(l, dim=0).unsqueeze(0) # [1,n,5]\n",
    "                src.append(z)\n",
    "                target.append(plist[j]['target'].unsqueeze(0))\n",
    "        src = torch.cat(src, dim=0).to(torch.float).to(device) # [batch_size, n, 5]\n",
    "        target = torch.cat(target, dim=-1).to(torch.long) # [batch_size]\n",
    "            \n",
    "        # LSTMによるテキスト＋価格の３値分類\n",
    "        _, x = self.lstm(src)\n",
    "        x = self.dense2(x[0].view(src.size(0), -1))\n",
    "\n",
    "        return x, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0419fd72-cc71-4ea6-9c03-14f8bc8118a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PositionalEncodingの概要\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        '''\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9367e57a-678c-4d2e-bc15-5fdfd4df01fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f38d9d46fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paramator for training & evaluation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "softmax = nn.Softmax(dim=1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a77848dc-a80f-4233-a46d-82d327d1f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(model, timespan, n, train_tdf, train_plist, epoch):\n",
    "    model.train()\n",
    "    train_test_flag = 0\n",
    "    \n",
    "    log_interval = math.ceil(train_num_batches/30)*10\n",
    "    batch_counter = 0\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_count = 0\n",
    "    \n",
    "    if timespan == '4h':\n",
    "        q = 4\n",
    "    elif timespan == '30m':\n",
    "        q = 9\n",
    "    else:\n",
    "        q = 25\n",
    "    \n",
    "    batch_start_time = time.time()\n",
    "    \n",
    "    for i in range(0, train_num_batches):\n",
    "        if (i+epoch)%q == 0:\n",
    "            if i*batch_size-n+1 >= 0:      \n",
    "                if i != (train_num_batches-1):\n",
    "                    t_start = i*batch_size-n+1\n",
    "                    t_end   = i*batch_size+batch_size\n",
    "                else:\n",
    "                    t_start = i*batch_size-n+1\n",
    "                    t_end   = len(train_plist)\n",
    "            else:\n",
    "                if i != (train_num_batches-1):\n",
    "                    t_start = 0\n",
    "                    t_end   = i*batch_size+batch_size\n",
    "                else:\n",
    "                    t_start = 0\n",
    "                    t_end   = len(train_plist)\n",
    "\n",
    "            predictions, target = model(timespan,n, t_start, t_end, train_tdf, train_plist, train_test_flag)\n",
    "            prob = softmax(predictions)\n",
    "            targets = target.to(device)\n",
    "            loss = criterion(predictions, targets)\n",
    "\n",
    "            correct = prob.argmax(axis=1) == targets\n",
    "            train_correct += correct.sum().item()\n",
    "            train_count += correct.size(0)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_counter += 1\n",
    "\n",
    "        if batch_counter % log_interval == 0 or batch_counter == train_num_batches:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            s_per_batch = (time.time() - batch_start_time) / log_interval\n",
    "            cur_loss = train_loss / log_interval\n",
    "            cur_acc = train_correct / train_count\n",
    "            print(f'| epoch {epoch:3d} | {batch_counter:5d}/{train_num_batches:5d} batches | '\n",
    "                  f'lr {lr:1.5f} | s/batch {s_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.3f} | accuracy {cur_acc:8.3f} |')\n",
    "            total_loss = 0\n",
    "            batch_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f793e1-ba75-49cf-8fe6-3a497f39d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation (val, test)\n",
    "def evaluate(model, timespan, n, test_tdf, test_plist, epoch):\n",
    "    model.eval()  \n",
    "    train_test_flag = 1\n",
    "    \n",
    "    eval_loss = 0\n",
    "    eval_correct = 0\n",
    "    eval_acc = 0\n",
    "    eval_count = 0\n",
    "    \n",
    "    if timespan == '4h':\n",
    "        q = 3\n",
    "        if epoch == -1:\n",
    "            q = 1\n",
    "    elif timespan == '30m':\n",
    "        q = 4\n",
    "        if epoch == -1:\n",
    "            q = 2\n",
    "    else:\n",
    "        q = 8\n",
    "        if epoch == -1:\n",
    "            q = 3\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, test_num_batches):\n",
    "            if (i+epoch)%q == 0:      \n",
    "                if i*batch_size-n+1 >= 0:      \n",
    "                    if i != (test_num_batches-1):\n",
    "                        t_start = i*batch_size-n+1\n",
    "                        t_end   = i*batch_size+batch_size\n",
    "                    else:\n",
    "                        t_start = i*batch_size-n+1\n",
    "                        t_end   = len(test_plist)\n",
    "                else:\n",
    "                    if i != (test_num_batches-1):\n",
    "                        t_start = 0\n",
    "                        t_end   = i*batch_size+batch_size\n",
    "                    else:\n",
    "                        t_start = 0\n",
    "                        t_end   = len(test_plist)\n",
    "\n",
    "                predictions, target = model(timespan,n, t_start, t_end, test_tdf, test_plist, train_test_flag)\n",
    "                prob = softmax(predictions)\n",
    "                targets = target.to(device)\n",
    "                loss = criterion(predictions, targets)\n",
    "                \n",
    "                eval_loss += loss.item()\n",
    "                correct = prob.argmax(axis=1) == targets\n",
    "                eval_correct += correct.sum().item()\n",
    "                eval_count += correct.size(0)\n",
    "                \n",
    "    eval_acc = eval_correct / eval_count\n",
    "    \n",
    "    print(f'| loss {eval_loss:5.3f}| accuracy {eval_acc:8.3f} |')\n",
    "     \n",
    "    return eval_loss, eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "499797d0-d06b-4c55-8768-3def2b2ee0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------timespan='4h' / n=2--------------------------------------------------\n",
      "2021-11-15 08:14:52.797787\n",
      "*********************************************training start*********************************************\n",
      "| epoch   1 |    20/   35 batches | lr 0.00100 | s/batch 103.53 | loss 0.286 | accuracy    0.306 |\n",
      "| epoch   1 |    35/   35 batches | lr 0.00100 | s/batch 61.80 | loss 0.454 | accuracy    0.324 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.358| accuracy    0.245 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1:03:51.466760s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch   2 |    20/   35 batches | lr 0.00095 | s/batch 103.45 | loss 0.285 | accuracy    0.294 |\n",
      "| epoch   2 |    35/   35 batches | lr 0.00095 | s/batch 66.38 | loss 0.503 | accuracy    0.306 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.448| accuracy    0.141 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 1:06:42.142238s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch   3 |    20/   35 batches | lr 0.00090 | s/batch 103.55 | loss 0.284 | accuracy    0.300 |\n",
      "| epoch   3 |    35/   35 batches | lr 0.00090 | s/batch 83.97 | loss 0.506 | accuracy    0.319 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.343| accuracy    0.254 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 1:12:35.898256s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch   4 |    20/   35 batches | lr 0.00086 | s/batch 103.28 | loss 0.287 | accuracy    0.233 |\n",
      "| epoch   4 |    35/   35 batches | lr 0.00086 | s/batch 82.96 | loss 0.507 | accuracy    0.296 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.326| accuracy    0.245 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 1:10:53.925440s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch   5 |    20/   35 batches | lr 0.00081 | s/batch 104.72 | loss 0.282 | accuracy    0.306 |\n",
      "| epoch   5 |    35/   35 batches | lr 0.00081 | s/batch 62.38 | loss 0.449 | accuracy    0.324 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.402| accuracy    0.141 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 1:06:15.394400s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch   6 |    20/   35 batches | lr 0.00077 | s/batch 104.32 | loss 0.282 | accuracy    0.294 |\n",
      "| epoch   6 |    35/   35 batches | lr 0.00077 | s/batch 67.02 | loss 0.499 | accuracy    0.306 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.315| accuracy    0.254 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 1:07:31.375869s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch   7 |    20/   35 batches | lr 0.00074 | s/batch 104.78 | loss 0.281 | accuracy    0.300 |\n",
      "| epoch   7 |    35/   35 batches | lr 0.00074 | s/batch 85.08 | loss 0.502 | accuracy    0.319 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.302| accuracy    0.245 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 1:12:18.768554s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch   8 |    20/   35 batches | lr 0.00070 | s/batch 104.27 | loss 0.283 | accuracy    0.233 |\n",
      "| epoch   8 |    35/   35 batches | lr 0.00070 | s/batch 84.27 | loss 0.503 | accuracy    0.296 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.368| accuracy    0.141 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 1:13:26.673895s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch   9 |    20/   35 batches | lr 0.00066 | s/batch 106.60 | loss 0.280 | accuracy    0.306 |\n",
      "| epoch   9 |    35/   35 batches | lr 0.00066 | s/batch 63.28 | loss 0.446 | accuracy    0.324 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.297| accuracy    0.254 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 1:07:04.254091s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch  10 |    20/   35 batches | lr 0.00063 | s/batch 105.85 | loss 0.280 | accuracy    0.294 |\n",
      "| epoch  10 |    35/   35 batches | lr 0.00063 | s/batch 67.87 | loss 0.497 | accuracy    0.306 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.284| accuracy    0.245 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 1:06:55.863391s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch  11 |    20/   35 batches | lr 0.00060 | s/batch 105.76 | loss 0.280 | accuracy    0.300 |\n",
      "| epoch  11 |    35/   35 batches | lr 0.00060 | s/batch 85.82 | loss 0.500 | accuracy    0.319 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.342| accuracy    0.141 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 1:14:27.698571s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch  12 |    20/   35 batches | lr 0.00057 | s/batch 104.64 | loss 0.281 | accuracy    0.233 |\n",
      "| epoch  12 |    35/   35 batches | lr 0.00057 | s/batch 84.21 | loss 0.500 | accuracy    0.296 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.283| accuracy    0.254 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 1:13:24.339207s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch  13 |    20/   35 batches | lr 0.00054 | s/batch 105.99 | loss 0.279 | accuracy    0.306 |\n",
      "| epoch  13 |    35/   35 batches | lr 0.00054 | s/batch 63.01 | loss 0.445 | accuracy    0.324 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.272| accuracy    0.245 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 1:05:19.368691s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch  14 |    20/   35 batches | lr 0.00051 | s/batch 105.69 | loss 0.279 | accuracy    0.294 |\n",
      "| epoch  14 |    35/   35 batches | lr 0.00051 | s/batch 67.92 | loss 0.496 | accuracy    0.306 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.321| accuracy    0.141 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 1:08:30.573982s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch  15 |    20/   35 batches | lr 0.00049 | s/batch 105.96 | loss 0.279 | accuracy    0.300 |\n",
      "| epoch  15 |    35/   35 batches | lr 0.00049 | s/batch 85.82 | loss 0.499 | accuracy    0.319 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.272| accuracy    0.254 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 1:14:22.249518s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch  16 |    20/   35 batches | lr 0.00046 | s/batch 105.23 | loss 0.280 | accuracy    0.233 |\n",
      "| epoch  16 |    35/   35 batches | lr 0.00046 | s/batch 84.55 | loss 0.499 | accuracy    0.296 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.262| accuracy    0.245 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 1:12:16.790003s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch  17 |    20/   35 batches | lr 0.00044 | s/batch 106.48 | loss 0.278 | accuracy    0.306 |\n",
      "| epoch  17 |    35/   35 batches | lr 0.00044 | s/batch 63.41 | loss 0.444 | accuracy    0.324 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loss 2.307| accuracy    0.141 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 1:07:14.740384s | val loss：1.000 | val accuracy：   2.000 |\n",
      "---------------------------------------------------------------------------------------\n",
      "| epoch  18 |    20/   35 batches | lr 0.00042 | s/batch 105.86 | loss 0.278 | accuracy    0.294 |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-78005efab94c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimespan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_plist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_cr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimespan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_plist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-960b042e462e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, timespan, n, train_tdf, train_plist, epoch)\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0mt_end\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_plist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimespan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_tdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_plist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_test_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-64f6fe6d84d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, timespan, n, t_start, t_end, tdf, plist, train_test_flag)\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, seq_len, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, seq_len, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, seq_len, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m#Nanを0に置き換え\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0msee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTransformer\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[0m\u001b[1;32m    294\u001b[0m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[1;32m    295\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    978\u001b[0m                 v_proj_weight=self.v_proj_weight)\n\u001b[1;32m    979\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m             return F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m    981\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4807\u001b[0m     \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4808\u001b[0;31m     \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4810\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main\n",
    "nlist = [2,3,4]\n",
    "tlist = ['4h','30m','5m']\n",
    "#tlist = ['5m','30m','4h']\n",
    "batch_size_list        = {'4h':32,  '30m':128, '5m':256}\n",
    "train_num_section_list = {'4h':1090,'30m':8722,'5m':52333}\n",
    "test_num_section_list  = {'4h':182, '30m':1454,'5m':8723}\n",
    "\n",
    "for n, timespan in itertools.product(nlist,tlist):\n",
    "\n",
    "    print('-'*50 + f'{timespan=} / {n=}' + '-'*50)\n",
    "    train_num_section = train_num_section_list[timespan]\n",
    "    test_num_section  = test_num_section_list[timespan]\n",
    "    batch_size=batch_size_list[timespan]\n",
    "    train_num_batches = (train_num_section // batch_size) +1\n",
    "    test_num_batches  = (test_num_section // batch_size) +1\n",
    "\n",
    "    lr = 1e-3\n",
    "    model = Net(ntokens, d_model, nhead, d_hid, nlayers, lstm_input_dim, lstm_hidden_dim, dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "    best_val_loss = float('inf')\n",
    "    epochs = 50\n",
    "    best_model = None\n",
    "    \n",
    "    train_tdf   = read_pickle_quickly(f'../../external_drive/pickle/{timespan}/train_tdf.pickle')\n",
    "    test_tdf    = read_pickle_quickly(f'../../external_drive/pickle/{timespan}/test_tdf.pickle')\n",
    "    train_plist = read_pickle_quickly(f'../../external_drive/pickle/{timespan}/train_plist_{n}.pickle')\n",
    "    test_plist  = read_pickle_quickly(f'../../external_drive/pickle/{timespan}/test_plist_{n}.pickle')\n",
    "\n",
    "    dt_start = datetime.datetime.now()\n",
    "    print(datetime.datetime.now())\n",
    "    print('*'*45 + 'training start' + '*'*45)\n",
    "\n",
    "    # training & test roop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start = datetime.datetime.now()\n",
    "\n",
    "        train(model, timespan, n, train_tdf, train_plist, epoch)\n",
    "        val_loss, val_acc = evaluate(model, timespan, n, test_tdf, test_plist, epoch)\n",
    "        \n",
    "        epoch_end = datetime.datetime.now()\n",
    "        elapsed = epoch_end - epoch_start\n",
    "\n",
    "        print('-' * 87)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed}s | val loss：{val_loss:5.3f} | val accuracy：{val_acc:8.3f} |')\n",
    "        print('-' * 87)\n",
    "        \n",
    "        # 結果をcsvに保存\n",
    "        df_log = pd.read_csv('tweet-transformer/loss_log.csv')\n",
    "        s = pd.Series([epoch_start,\n",
    "                       epoch_end,\n",
    "                       elapsed,\n",
    "                       timespan,\n",
    "                       n,\n",
    "                       epoch,\n",
    "                       val_loss,\n",
    "                       val_acc],\n",
    "                index=['start_time',\n",
    "                       'end_time',\n",
    "                       'elapsed',\n",
    "                       'timespan',\n",
    "                       'range_of_data',\n",
    "                       'epoch',\n",
    "                       'val_loss',\n",
    "                       'val_accuracy'])\n",
    "        df_log = df_log.append(s, ignore_index=True)\n",
    "        df_log.to_csv('tweet-transformer/loss_log.csv',index=False)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    dt_end = datetime.datetime.now()\n",
    "    elapsed = dt_end - dt_start\n",
    "    print(datetime.datetime.now())    \n",
    "    print('*'*30 + f'Finish! training time：{elapsed}s' + '*'*30)\n",
    "\n",
    "    # test\n",
    "    test_loss, test_acc = evaluate(best_model, timespan, n, test_tdf, test_plist, -1)\n",
    "    print('=' * 89)\n",
    "    print(f'| End of training | test loss：{test_loss:5.3f} | '\n",
    "          f'test accuracy：{test_acc:8.3f}')\n",
    "    print('=' * 89)\n",
    "    \n",
    "    # 結果をcsvに保存\n",
    "    df_log = pd.read_csv('tweet-transformer/transformer_log.csv')\n",
    "    s = pd.Series([dt_start,\n",
    "                   dt_end,\n",
    "                   elapsed,\n",
    "                   timespan,\n",
    "                   n,\n",
    "                   test_loss,\n",
    "                   test_acc],\n",
    "            index=['start_time',\n",
    "                   'end_time',\n",
    "                   'elapsed',\n",
    "                   'timespan',\n",
    "                   'range_of_data',\n",
    "                   'test_loss',\n",
    "                   'test_accuracy'])\n",
    "    df_log = df_log.append(s, ignore_index=True)\n",
    "    df_log.to_csv('tweet-transformer/transformer_log.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ddcf0-6f22-4e70-ade4-b0842dddbd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92469a24-b45c-4cff-b27a-26dc385dbc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26b8163e-c26d-4c9d-8b9f-ff6bb147d50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device0 = torch.device(\"cuda:0\")\n",
    "device1 = torch.device(\"cuda:1\")\n",
    "print(torch.cuda.is_available())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687778d9-7143-4c17-b8a3-d29cdcd570c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.getsizeof(x))\n",
    "# これでtrain_tlistなどを読み込んだ時の容量を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbc945f8-538d-49a7-9c6f-2cf446a6b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log リセット\n",
    "'''\n",
    "# test用\n",
    "df = pd.DataFrame(columns=['start_time',\n",
    "                           'end_time',\n",
    "                           'elapsed',\n",
    "                           'timespan',\n",
    "                           'range_of_data',\n",
    "                           'test_loss',\n",
    "                           'test_accuracy'])\n",
    "df.to_csv('tweet-transformer/transformer_log.csv',index=False)\n",
    "\n",
    "# epoch毎のvalidation用\n",
    "df = pd.DataFrame(columns=['start_time',\n",
    "                           'end_time',\n",
    "                           'elapsed',\n",
    "                           'timespan',\n",
    "                           'range_of_data',\n",
    "                           'epoch',\n",
    "                           'val_loss',\n",
    "                           'val_accuracy'])\n",
    "df.to_csv('tweet-transformer/loss_log.csv',index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9837880-669a-48d4-8ac7-27d1157630f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
