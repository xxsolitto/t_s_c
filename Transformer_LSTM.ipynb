{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "425ceb85-d9ff-409a-880f-1d0f94c562c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import data\n",
    "from torchtext.legacy import data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e604666-0df1-4b16-9463-2d7f0f8d03c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ読み込み中\n",
      "データ読み込み完了\n",
      "len(vocab)=230011\n",
      "len(vocab)=404095\n",
      "len(vocab)=646245\n",
      "len(vocab)=1019658\n",
      "len(vocab)=3415818\n",
      "辞書作成完了\n",
      "511.6372814178467 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3711"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vacab作成\n",
    "# テキストを単語で分割\n",
    "v_start = time.time()\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# data field定義\n",
    "TEXT  = data.Field(sequential=True,\n",
    "                     lower=True,\n",
    "                     batch_first=True, \n",
    "                     tokenize=tokenizer,\n",
    "                     init_token='<cls>')\n",
    "\n",
    "print(\"データ読み込み中\")\n",
    "# CSVファイルを読み込み、TabularDatasetオブジェクトの作成\n",
    "vocab_data = data.TabularDataset(path ='tweet-transformer/1d/2021-17.csv',\n",
    "                                       format='csv',\n",
    "                                       skip_header = True,\n",
    "                                       fields=[('tweet', TEXT)])\n",
    "print(\"データ読み込み完了\")\n",
    "\n",
    "# 単語辞書の作成\n",
    "TEXT.build_vocab(vocab_data, min_freq=3)\n",
    "vocab = TEXT.vocab\n",
    "print(f'{len(vocab)=}')\n",
    "'''\n",
    "TEXT.build_vocab(vocab_data, min_freq=10)\n",
    "vocab = TEXT.vocab\n",
    "print(f'{len(vocab)=}')\n",
    "TEXT.build_vocab(vocab_data, min_freq=5)\n",
    "vocab = TEXT.vocab\n",
    "print(f'{len(vocab)=}')\n",
    "TEXT.build_vocab(vocab_data, min_freq=2)\n",
    "vocab = TEXT.vocab\n",
    "print(f'{len(vocab)=}')\n",
    "TEXT.build_vocab(vocab_data, min_freq=1)\n",
    "vocab = TEXT.vocab\n",
    "print(f'{len(vocab)=}')\n",
    "'''\n",
    "\n",
    "print('辞書作成完了')\n",
    "print(f'{time.time() - v_start:5.2f} s')\n",
    "\n",
    "# メモリ開放\n",
    "del v_start, vocab_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c74147d-a4f4-4872-b80e-24706ca59574",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# データの前処理\n",
    "def data_process(timespan):\n",
    "    train_iter_list = []\n",
    "    val_iter_list = []\n",
    "    test_iter_list = []\n",
    "\n",
    "    klist = {'1d' :211+1,\n",
    "             '12h':423+1,\n",
    "             '4h' :1271+1,\n",
    "             '1h' :5087+1, \n",
    "             '30m':10175+1,\n",
    "             '15m':20351+1,\n",
    "             '5m' :61055+1}\n",
    "    num_sections = klist[timespan]\n",
    "    log_interval = math.ceil(num_sections/100)*10\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # テキストを単語で分割\n",
    "    #tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "    # data field定義\n",
    "    SECTION = data.Field(sequential=False,\n",
    "                         use_vocab=False)\n",
    "\n",
    "\n",
    "    # CSVファイルを読み込み、TabularDatasetオブジェクトの作成\n",
    "    k = klist[timespan]\n",
    "    for i in range(0,k):\n",
    "        all_dataset = data.TabularDataset(path =f'tweet-transformer/{timespan}/mini-batches/section_{i}.csv',\n",
    "                                               format='csv',\n",
    "                                               skip_header = True,\n",
    "                                               fields=[('tweet', TEXT),\n",
    "                                                       ('section', SECTION)])\n",
    "\n",
    "        # trainとtestで分割\n",
    "        train_val_dataset, test_dataset = all_dataset.split(split_ratio=6/7)\n",
    "        train_dataset, val_dataset = train_val_dataset.split(split_ratio=5/6, random_state=random.seed(1234))\n",
    "\n",
    "        # テキストを数値ベクトル化、バッチに分割\n",
    "        batch_size = 128\n",
    "        train_iter,val_iter,test_iter = data.BucketIterator.splits((train_dataset,val_dataset,test_dataset),\n",
    "                                                                   batch_sizes=(batch_size,batch_size,batch_size),\n",
    "                                                                   sort = False,\n",
    "                                                                   shuffle=True)\n",
    "        \n",
    "        train_iter_list.append(train_iter)\n",
    "        val_iter_list.append(val_iter)\n",
    "        test_iter_list.append(test_iter)\n",
    "        \n",
    "        # メモリ解放\n",
    "        del all_dataset,train_dataset,train_iter,val_dataset,val_iter,test_dataset,test_iter\n",
    "        gc.collect()\n",
    "        \n",
    "        counter += 1\n",
    "        if counter % log_interval == 0 or counter == num_sections:\n",
    "            print(f'|{counter:5d}/{num_sections:5d} sections | ')\n",
    "        \n",
    "    print(f'読み込み完了　　{timespan} : {time.time()-start_time} s')\n",
    "    \n",
    "    return train_iter_list, val_iter_list, test_iter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9406e05f-80e9-4f74-92d8-409d156844a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetの定義\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, x, y, tokenizer, vacab, max_len):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):  # len(Dataset)で返す値を指定\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
    "        text = self.x[index]\n",
    "        text = self.tokenizer(text)\n",
    "        ids = torch.tensor([self.vocab[word] for word in text], dtype=torch.long) # [seq_len]\n",
    "        ids = F.pad(ids, (0 ,max_len), \"constant\", 0) # [max_len]\n",
    "        mask = [ids==0]\n",
    "\n",
    "        section = self.y[index]\n",
    "\n",
    "        return {'ids'   : ids,\n",
    "                'mask'  : mask,\n",
    "                'section': torch.Tensor([section])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d47a3777-0423-4ea8-a3bd-7fd9421abb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetの作成\n",
    "def dataset(vocab,timespan):\n",
    "    max_len = 128\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "    # CSVファイルを読み込み、データセットを作成\n",
    "    df = pd.read_csv(f'tweet-transformer/{timespan}/2021-17.csv')\n",
    "    dataset = CreateDataset(df['tweet(n)'],  df['section'],  tokenizer, vocab, max_len)\n",
    "\n",
    "    print('dataset作成完了')\n",
    "    print(f'{len(dataset)=}')\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8bb32-126c-47f6-a07c-0ae402aadbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# セクションごとにバッチ化 \n",
    "def section_batch(dataset):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac7c4b-bf7e-49ef-a2a4-b7bcc81e932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nごとにバッチ化 \n",
    "def n_batch(sbatches):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f7afac-54ab-4475-b276-d3b7cd2fa0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametator for Net\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "d_model = 512  # embedding dimension\n",
    "nhead   = 8    # number of heads in nn.MultiheadAttention\n",
    "d_hid   = 2048  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 6    # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "dropout = 0.2  # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4024d1bf-ade2-4b1a-987c-e759fb32d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerモデルの概要\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 ntoken: int,\n",
    "                 d_model: int,\n",
    "                 nhead: int,\n",
    "                 d_hid: int,\n",
    "                 nlayers: int,\n",
    "                 dropout: float = 0.5):\n",
    "\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "        self.emb = nn.Embedding(ntoken,\n",
    "                                d_model,\n",
    "                                padding_idx=0)\n",
    "        self.pos_encoder = PositionalEncoding(d_model,\n",
    "                                              dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model,\n",
    "                                                 nhead,\n",
    "                                                 d_hid,\n",
    "                                                 dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers,\n",
    "                                                      nlayers)\n",
    "        self.classifer = nn.Linear(d_model,3)\n",
    "        #self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.input_dim = lstm_input_dim\n",
    "        self.hidden_dim = lstm_hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=lstm_input_dim, \n",
    "                            hidden_size=lstm_hidden_dim,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True\n",
    "                            )\n",
    "        \n",
    "        \n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.classifer.bias.data.zero_()\n",
    "        self.classifer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    #データの流れ\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            src: Tensor, shape [batch_size, seq_len]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [batch_size, nclass:3]\n",
    "        '''\n",
    "# src_key_padding_mask = src_mask\n",
    "        \n",
    "#        for i in range(1,num_batches+1)\n",
    "            embedded = self.emb(src) * math.sqrt(self.d_model)\n",
    "            pos = self.pos_encoder(embedded)\n",
    "            encoder_out = self.transformer_encoder(pos)\n",
    "            x = encoder_out.mean(dim=1)\n",
    "            output = self.classifer(x)\n",
    "            \n",
    "        \n",
    "        #output = self.softmax(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0419fd72-cc71-4ea6-9c03-14f8bc8118a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PositionalEncodingの概要\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        '''\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9367e57a-678c-4d2e-bc15-5fdfd4df01fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x245622117b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paramator for training & evaluation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(ntokens, d_model, nhead, d_hid, nlayers, dropout).to(device)\n",
    "lr = 1e-3\n",
    "softmax = nn.Softmax(dim=1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a77848dc-a80f-4233-a46d-82d327d1f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(model: nn.Module, train_iter: Tensor):\n",
    "    train_start_time = time.time()\n",
    "    model.train()\n",
    "    num_batches = len(train_iter)\n",
    "    log_interval = math.ceil(num_batches/30)*10\n",
    "    batch_counter = 0\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_count = 0\n",
    "    \n",
    "    for idx, batch in enumerate(iter(train_iter)):\n",
    "        predictions = model(batch.tweet_n.to(device))\n",
    "        prob = softmax(predictions)\n",
    "        labels = batch.label.to(device)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        correct = prob.argmax(axis=1) == labels\n",
    "        acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "        train_correct += correct.sum().item()\n",
    "        train_count += correct.size(0)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_counter += 1\n",
    "        \n",
    "        if batch_counter % log_interval == 0 or batch_counter == num_batches:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            s_per_batch = (time.time() - train_start_time) / log_interval\n",
    "            cur_loss = train_loss / log_interval\n",
    "            cur_acc = train_correct / train_count\n",
    "            print(f'| epoch {epoch:3d} | {batch_counter:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:1.5f} | s/batch {s_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | accuracy {cur_acc:8.2f}')\n",
    "            total_loss = 0\n",
    "            train_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f793e1-ba75-49cf-8fe6-3a497f39d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation (val, test)\n",
    "def evaluate(model: nn.Module, eval_iter: Tensor):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_correct = 0\n",
    "    eval_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(iter(eval_iter)):\n",
    "            predictions = model(batch.tweet_n.to(device))\n",
    "            prob = softmax(predictions)\n",
    "            labels = batch.label.to(device)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            correct = prob.argmax(axis=1) == labels\n",
    "            acc = correct.sum().item() / correct.size(0)\n",
    "\n",
    "            eval_correct += correct.sum().item()\n",
    "            eval_count += correct.size(0)\n",
    "            eval_loss += loss.item()\n",
    "        \n",
    "    print(f'| loss {eval_loss}| accuracy {eval_correct / ecal_count} ')\n",
    "        \n",
    "    return eval_loss, eval_correct / eval_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499797d0-d06b-4c55-8768-3def2b2ee0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "nlist = [1,2,3,4,5,6,7,8,9,10]\n",
    "tlist = ['1d','12h','4h','1h','30m','15m','5m']\n",
    "aculist = {}\n",
    "for timespan in tlist:\n",
    "    print(f'{timespan=} データ読み込み中')\n",
    "    train_iter_list, val_iter_list, test_iter_list = data_process(timespan)\n",
    "    \n",
    "    for n in nlist:\n",
    "        print(f'{n=}')\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        epochs = 1\n",
    "        best_model = None\n",
    "\n",
    "        dt_start = datetime.datetime.now()\n",
    "        print(datetime.datetime.now())\n",
    "        print('学習開始')\n",
    "        print('-' * 95)\n",
    "\n",
    "        # training & validation roop\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            train(model, train_iter_list)\n",
    "            val_loss, val_acc = evaluate(model, val_iter_list)\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            print('-' * 95)\n",
    "            print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "                  f'valid loss {val_loss:5.2f} | valid accuracy {val_acc:8.2f}')\n",
    "            print('-' * 95)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            del epoch_start_time, elapsed, vall_loss, vall_acc\n",
    "            gc.collect()\n",
    "\n",
    "        dt_end = datetime.datetime.now()\n",
    "        print(datetime.datetime.now())    \n",
    "        print(f'経過時間：{dt_end - dt_start}')\n",
    "        print('学習終了')\n",
    "        \n",
    "        # test\n",
    "        test_loss, test_acc = evaluate(best_model, test_iter_list)\n",
    "        print('=' * 89)\n",
    "        print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "              f'test accuracy {test_acc:8.2f}')\n",
    "        print('=' * 89)\n",
    "        \n",
    "        del best_val_loss,epochs,best_model,test_loss,test_acc,dt_start,dt_end\n",
    "        gc.collect()\n",
    "        \n",
    "    del train_iter_list, val_iter_list, test_iter_list\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9b279-ca38-4d79-a472-ca2529ca9bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58297e-125a-404c-b1ff-1808669dbed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ec3f1-3382-4c73-8e21-b436c35c5e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf691823-0891-4e28-87ea-82a5dff263ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3844708a-0b51-4389-8cc0-ea15ea89e9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.legacy.data.batch.Batch of size 128]\n",
      "\t[.tweet_n]:[torch.LongTensor of size 128x61]\n",
      "\t[.label]:[torch.LongTensor of size 128]\n",
      "tensor([[   2,  821,   58,  ...,    1,    1,    1],\n",
      "        [   2,    0,    0,  ...,    1,    1,    1],\n",
      "        [   2,  270,  244,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,   11,   23,  ...,    1,    1,    1],\n",
      "        [   2, 1709, 1591,  ...,    1,    1,    1],\n",
      "        [   2,   18,    6,  ...,    1,    1,    1]])\n"
     ]
    }
   ],
   "source": [
    "train_ = next(iter(train_iter))\n",
    "tweet = train_.tweet_n\n",
    "print(train_)\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c45f8e-bbca-4afb-9819-989b38d02275",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "#l=len(train_iter)\n",
    "for idx, batch in enumerate(iter(train_iter)):\n",
    "    i+=1\n",
    "    print(idx)\n",
    "#    print(batch.label)\n",
    "    print(batch.tweet_n)\n",
    "    print(batch.tweet_n.size())\n",
    "    if i==3: break\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b8163e-c26d-4c9d-8b9f-ff6bb147d50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810af7d7-cdf9-4b90-8ae2-78bb8d8e3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_list, val_iter_list, test_iter_list = data_process('1d')\n",
    "print(len(train_iter_list))\n",
    "print(len(val_iter_list))\n",
    "print(len(test_iter_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f78dab-5daf-46fa-a302-8f07287bb5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1,10):\n",
    "    print('-'*90)\n",
    "    for batch in iter(test_iter_list[i]):\n",
    "        print(batch)\n",
    "        print(batch.section)\n",
    "        print(batch.tweet)\n",
    "        print(batch.tweet.size())\n",
    "    if i == 3: break\n",
    "print(\"end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73bacd38-e9eb-4a6a-955a-64412824b760",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-023baac0d4ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m13140000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(df_dataset))\n",
    "print(df_dataset[13140000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a507096-d5fc-494e-a62c-32c6c6118ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019658\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(vocab['<cls>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95ed96d0-c49f-472a-b945-0a747161b89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df)=13146832\n",
      "dataset作成完了\n",
      "len(dataset)=13146832\n"
     ]
    }
   ],
   "source": [
    "# テスト用\n",
    "max_len = 128\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# CSVファイルを読み込み、データセットを作成\n",
    "df = pd.read_csv(f'tweet-transformer/1d/2021-17.csv')\n",
    "print(f'{len(df)=}')\n",
    "dataset = CreateDataset(df['tweet(n)'],  df['section'],  tokenizer, vocab, max_len)\n",
    "\n",
    "print('dataset作成完了')\n",
    "print(f'{len(dataset)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "825f0db1-3f34-400e-ba42-c81b57584aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55276289-3dd1-430d-9c3d-d121a3394409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]])\n",
      "torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,128)\n",
    "y=torch.ones(1,128)\n",
    "z=torch.cat([x,y,x,y],0)\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c940bc2-4c59-4a13-b2cc-4f2826291212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd9d930b-c4e0-497b-838e-32b59adabb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140])\n",
      "torch.Size([157])\n",
      "torch.Size([146])\n",
      "torch.Size([172])\n",
      "torch.Size([153])\n",
      "torch.Size([158])\n",
      "torch.Size([173])\n",
      "torch.Size([162])\n",
      "torch.Size([141])\n",
      "torch.Size([158])\n",
      "tensor([  1333,   1345,   3124,   8816,   5406,   8672,      3,  85290,  19615,\n",
      "             3,  82409,  25114,      3, 105019,  31599,      3, 203741,   5406,\n",
      "           973,      3, 112693,   5741,      3,  79954,   1209,      3,  97827,\n",
      "           809,      3,   3494,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0])\n",
      "torch.Size([158])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "#    if i == 0:\n",
    "    z = dataset[i]['ids']\n",
    "    print(z.size())\n",
    "#    else:\n",
    "#        z = torch.cat([z,dataset[i]['ids']],0)\n",
    "print(z)\n",
    "print(z.size())\n",
    "print(z.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d215e61d-9d43-49fc-aae4-ca09db7dfb1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 128]' is invalid for input of size 1560",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f9d7109f7355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 128]' is invalid for input of size 1560"
     ]
    }
   ],
   "source": [
    "z.view(-1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd6626-128c-4f2a-be50-599479e226f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
